\section{Conclusion and Future Work} \label{sec:conclusion}
% Primary: Aur√©lien

As we build larger and possibly more error prone platforms, being able to
protect applications against platform faults in a timely and efficient manner,
will be critical to maintaining performance. Current production
fault-protection techniques rely on checkpoint/restart to ensure such progress.
However, these techniques, by their very nature, regularly save the application state
to stable storage, and therefore increase the burden of the already overtaxed
I/O subsystem.

In this work we presented a comprehensive model of interference between
applications performing I/O on a shared platform. We designed and assessed
multiple I/O scheduling algorithms, with the goal if improving the overall
throughput of the platform. We formulated a steady-state analysis, which is
similar to platform usage over long periods of time, and proved a theoretical
baseline for achievable performance for I/O constrained checkpointing
workloads.  Unsurprisingly, the gain is most prominent on platforms with a
challenging MTBF or under-provisioned I/O; our heuristic invariably improves
the platform efficiency. Through simulation, we show a path to supporting
application-level checkpointing while maintaining a 80\% platform efficiency,
all without a large investment in the I/O partition.

% In this paper we presented a comprehensive model to capture interference
% between multiple applications performing fault-tolerance related I/O
% on a shared HPC system. We designed multiple algorithms
% to schedule and order the checkpointing I/O workload, with the intent of
% diminishing the average slowdown sustained by applications on the
% platform induced by sharing the I/O subsystem, \ie improve the throughput
% of the platform. We formulated a steady-state analysis of a scenario
% where CR-CR interference is avoided, which helps us
% define a theoretical baseline for achievable performance in I/O
% constrained checkpointing workloads.
% We designed a event-based simulator that permits
% executing typical HPC workloads on current and prospective systems.
% With this simulator we have been able to demonstrate that our proposed
% heuristic improves the platform efficiency. Unsurprisingly the gain is
% more marked on platforms with a challenging MTBF or with
% under-provisioned I/O, but our heuristic improves the efficiency in
% all cases.  We also simulated the situation on a not yet available
% platform, this time with the goal of providing guidance in the
% general I/O requirements for future HPC systems to be able to
% sustain checkpointing with the desired 80\% efficiency, a goal that
% we have found achievable with a third of the I/O aggregate bandwidth
% requirements when the system employs a smart checkpointing policy.

As burst-buffers and other NVRAM storage are becoming more common, a natural
extension would be to consider the impact on I/O contention/interference with
nodes sharing a common burst-buffer. Increasing the available I/O bandwidth
leads to reduced the waste (due to the decrease in checkpoint duration but also
an increase in the optimal checkpoint frequency and therefore a decrease in the
restart time), while providing relief to the shared I/O subsystem to better
absorb additional checkpoint information. We speculate that scheduling the
commits to the PFS with a heuristic that prioritizes applications whose loss
would be more costly can play a role in improving the efficiency of the entire
burst-buffer system.

% As burst-buffers and other NVRAM storage are becoming more common
% in PFS architecting, a natural extension of this work is to consider the effect
% of I/O contention/interference with hierarchies, in which subgroups of
% nodes (\eg a cabinet) may share a burst buffer, and thus experience
% interference for I/O in that same group, but be immune to interferences
% from I/O from other groups. Another interesting point with burst-buffers,
% is that space availability, in addition to bandwidth, may become
% contentious. The speed at which the burst-buffers can be committed to
% the sink PFS (possibly creating interference between multiple burst-buffers being
% flushed to the sink PFS simultaneously) now interplays with the
% optimal checkpoint frequency of applications, and can cause some
% applications running out of burst-buffer space. Again, we postulate that
% scheduling the commits to the PFS sink with an heuristic that prioritizes
% applications whose loss in failure cases would be more costly
% can play a role in improving the efficiency of the whole burst-buffers
% system.
