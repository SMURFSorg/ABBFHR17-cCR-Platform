% !TEX root =  ipdps18.tex

\section{Results}\label{sec:results}
% Primary: Thomas & all

\subsection{LANL APEX Simulation Workflows on Cielo}

We consider the workload from LANL found in the APEX Workflows
report~\cite{apex2016} that consists of four applications
classes: EAP, LAP, Silverton and VPIC. The main characteristics of
these classes are reported in Table~\ref{table:lanl}. We simulate the
behavior of these applications over the Cielo Platform. Cielo is a
1.37 Petaflops capability system installed in 2010-2011 at the Los
Alamos National Laboratory, with 143,104 cores, 286 TB of main memory,
and a parallel filesystem with a theoretical maximum capacity of 160GB/s.

\begin{table}
\begin{tabular}{|l|c|c|c|c|}
\hline
 Workflow & EAP & LAP & Silverton & VPIC \\\hline
Workload percentage & 66 & 5.5 & 16.5 & 12 \\\hline
Work time (h) & 262.4 & 64 & 128 & 157.2 \\\hline
Number of cores & 16384 & 4096 & 32768 & 30000 \\\hline
Initial Input (\% of memory) &  3 & 5 & 70 & 10 \\\hline
Final Output (\% of memory) & 105 & 220 & 43 & 270 \\\hline
Checkpoint Size (\% of memory) & 160 & 185 & 350 & 85 \\\hline
\end{tabular}
\caption{LANL Workflow Workload from the APEX Workflows report.\label{table:lanl}}
\end{table}

Our comparison baseline comprises a set of simulations with no faults, checkpoints,
nor I/O interference. For these simulations, we selected a 60-day execution segment,
and computed the resources used by the jobs during this period, \ie the total time
each node spent on (non-CR) I/O and computation in a failure-free environment.

Then, for the I/O scheduling techniques presented in
Section~\ref{sec:algorithms}, we computed the resource waste as the
total time computing nodes spent not progressing jobs. In the
results figures, we represent the performance of each strategy by computing
the waste ratio, \ie the resource waste over a segment of 60 days
divided by the application resource usage over that same segment for
the baseline simulation. Each simulation is conducted at least 1,000
times; the candlestick extremes represent the first and last decile of
the measures, while the boxes represent the second and third quartile,
and the point in the middle is the mean value. \dca{if 1st and last decile goes from
  10-90\%, shouldn't it also be 1st and last quartile, from 25-75\%?}

\begin{figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{sim/figures/synthetic-01hMTBF-waste-cielo.tex}}
  \end{center}
  \caption{Waste ratio as a function of the system bandwidth for the
    seven I/O and Checkpointing scheduling strategies, and the LANL workload on
    Cielo. \label{fig:cielo-1hmtbf}}
\end{figure}

\paragraph{The Impact of Available System Bandwidth}
First, we explore the performance of each approach under high risks
of failures. Figure~\ref{fig:cielo-1hmtbf} represents the waste ratio
on Cielo, assuming the node MTBF $\muind$ was 2 years (i.e. a system
MTBF of 1h). We vary the filesystem bandwidth from 40 GB/s to 160GB/s
in order to evaluate the impact of this parameter. We observe three
classes of behavior: \propfixed and \bfifofixed exhibit a waste ratio
that decreases as the bandwidth increases, but remains above 40\% even
at the maximum theoretical I/O bandwidth; \fifodaly, \fifofixed, and
\cooperative quickly decrease to below 20\% of waste, and reach
the theoretical model performance;
%
\footnote{Maple code to compute the
  performance predicted by the theoretical model is available on
  \url{https://github.com/SMURFSorg/InterferingCheckpoints}}
%
and \propdaly and \bfifodaly start at the same level of efficiency as
\propfixed and \bfifofixed, and slowly reach 20\% of waste as the bandwidth
increases.
%
Note that in some cases, the error bars below the theoretical
lower bound. In the simulations, failures have an exponential probability
distribution centered around the desired MTBF. In some runs, a lower
number of failures experienced during the simulation results in a larger
MTBF than the average used in the lower-bound formula, and such instances
can experience a waste lower than the theoretical model.

This figure shows that with a high frequency of failures, providing each job
with the appropriate checkpoint interval is paramount to preventing unnecessary (or
even detrimental) checkpoints: the two strategies that render high waste
despite a high bandwidth rely on a fixed 1h interval. But it also shows that
this is not the sole criteria that should be taken into account, nor a necessary
condition to extract performance. Even for favorable bandwidth, \propdaly and
\bfifodaly experience close to twice the waste of the other strategies relying
on the same value for the checkpointing period. All strategies that decouple the
execution of the application from the filesystem availability (\fifodaly,
\fifofixed, \cooperative) exhibit much better performance despite low bandwidth.

Notably, \cooperative remains the most efficient technique under all
conditions, and reaches the theoretical performance given by
Equation~\eqref{eq.totalwaste} for steady-state analysis. This
illustrates the efficiency of the proposed heuristic
(Equations~\eqref{eq.selection} and~\eqref{eq.selection2}) to schedule
checkpoints and I/O in a way that avoids interferences, allowing the
system to behave as if no interference was experienced, in most
cases. The high variation shows that a minority of the runs
experienced a significantly higher waste, but such is the case for
all algorithms.

\begin{figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{sim/figures/synthetic-040gbs-waste-cielo.tex}}
  \end{center}
  \caption{Waste ratio as a function of the system MTBF for the
    seven I/O and Checkpointing scheduling strategies, and the LANL workload on
    Cielo. \label{fig:cielo-40gbs}}
\end{figure}

\paragraph{The Impact of System Reliability}
Second, we explore the performance of each approach under low bandwidth (and
thus high risk of interference). A scenarios with such low bandwidth is not
unrealistic, as mentioned in Luu et al~\cite{Luu:2015:Multiplatform} practical
bandwidth can be consideraly lower than the theoretical.
Figure~\ref{fig:cielo-40gbs} represents the waste ratio on Cielo, assuming the
aggregated filesystem bandwidth of the system was 40GB/s. We vary the node MTBF
$\muind$ from 2 years (1h of system MTBF) to 50 years (24h of system MTBF) in
order to evaluate the impact of this parameter. Similarly to
Figure~\ref{fig:cielo-1hmtbf}, we observe three classes of behavior: \propfixed
and \bfifofixed exhibit a waste ratio that remains constant around 80\% for all
values of the MTBF. These approaches are critically dependent on the filesystem
bandwidth, and a lower frequency of failures does not significantly improve
their performance. The I/O subsystem is saturated, and the applications spends
most of their time waiting for it.
%
\propdaly and \bfifodaly, see poor efficiency for small MTBF values, but
steadily improve to come close to the theoretical bound for higher MTBF values.
Last, \fifodaly, \fifofixed, and \cooperative quickly reach the theoretical
model performance, even with a challenging MTBF (4 years of node MTBF; or 2h of
system MTBF).

For all the strategies that integrate the Daly checkpointing period
optimization, increasing the MTBF reduces the amount of I/O required
and thus relieves the pressure of a constrained bandwidth. All
strategies that schedule the bandwidth are successful at increasing the
efficiency close to the theoretical model.
%
Similarly to the previous result, \fifofixed, despite its fixed checkpoint
interval is capable of reaching a performance comparable to the Daly-based
strategies that reduce the number of checkpoints. The quick improvement of the
\fifofixed approach can be explained by a combination of 2 factors. Foremost,
the non-blocking aspect of the checkpoint, provide the I/O subsystem with enough
flexibility to order the checkpoint without imposing additional wait. Delayed
checkpoints only translate in additional waste if that application itself is
subject to failure. Additionally, for small MTBFs, the more frequent restarts of
other jobs despite the fact that they delay the checkpointing operation do not
introduce additional waste.

%% HT + GB: we wanted to say the same thing but in a more clear setting
% Surprisingly so in the case of \fifofixed, with its fixed checkpoint
% interval, which is capable of reaching a performance
% comparable to the strategies that reduce the number of
% checkpoints. At 2h of system MTBF (4 years of node MTBF), the
% supplementary I/O from restarting processes competes with the high
% fixed checkpoint frequency for scarce I/O resources, resulting in
% significant wastage. However, at 8h of system MTBF (16 years of node
% MTBF), the number of restarts is greatly reduced and the non blocking
% checkpointing approach is sufficient to even the I/O load efficiently.

\subsection{Evaluating a Prospective System}

\begin{figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{sim/figures/prosp.tex}}
  \end{center}
  \caption{Minimum aggregated filesystem bandwidth to reach 80\%
    efficiency with the different approaches on the prospective
    future system.\label{fig:prosp}}
\end{figure}

In order to understand the impact of the I/O contention on future platforms, we
use our simulator to explore a prospective system and assess the impact of I/O
and checkpoint scheduling when the problem size and the machine size will
increase. We consider a future system with 7PB of main memory and 50,000 compute
nodes (Aurora\footnote{\url{https://aurora.alcf.anl.gov/}}). Based on the APEX
workflow report, we extrapolate the increase in problem size expected for the
application classes considered above, and project these applications on the
prospective system.  We simulate the workload of Table~\ref{table:lanl}, scaling
the problem size proportionally to the change in machine memory size. The waste
is computed, as previously, by dividing the amount of resource used for
checkpoints and lost due to failures by the amount of resource used in a
fault-free and resilience-free run with the same initial conditions.
%
We consider different values for the system MTBF; and for each strategy, we
find the required aggregated practical bandwidth necessary to provide a
sustained 80\% efficiency of the system.
%
Figure~\ref{fig:prosp} shows the impact of the MTBF and of the
different strategies on this system.

When the failures are frequent (under 10 years of node MTBF), the most critical
element is to reduce the I/O pressure: all strategies that use a fixed and
frequent checkpoint interval require a much higher system bandwidth to reach the
target efficiency.  In this case, strategies that combine an optimal
checkpointing period with I/O and checkpoint scheduling (\cooperative and
\fifodaly) perform similarly, consistently better than all other approaches.
These 2 approaches exhibit a strong resilience to failures, with a bandwidth
requirement that only increases by a factor 3 between a very unstable system (at
less than 1h of system MTBF), and a stable one (at 8h of system MTBF). On the
contrary, the other strategies are much more dependent upon the frequency of
failures, with the \propfixed strategy requiring up to 50 times the bandwidth of
\cooperative to reach the same efficiency under these MTBF conditions.

When failures are not an endemic
event, \ie as soon as the node MTBF is at least 15 years
(corresponding to a system MTBF of 2.6h), the hierarchy of different
approaches stabilizes: the two blocking strategies that rely on
frequent checkpoints (\propfixed and \bfifofixed) are the ones
requiring the highest bandwidth to maintain 80\%
efficiency. % 6.4TB/s at 24 years
\fifofixed, that uses a fixed checkpoint interval comes next, with a
requirement around three quarter of the one required by \propfixed and
\fifofixed. It benefits from the capacity to continue working when the
filesystem is not available to checkpoint, which is sufficient, when
failures are rare, to obtain a significant performance
gain. % 4.9TB/s at 24 years
Then the four strategies that use the Daly optimal checkpointing
period, alleviating significantly the I/O pressure, are the ones that
require the lowest bandwidth to reach 80\% of efficiency, at about
half the required bandwidth of the current
situation. %coop: 2.9 TB/s at 24 years
