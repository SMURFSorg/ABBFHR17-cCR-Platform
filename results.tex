% !TEX root =  ipdps18.tex

\section{Results}\label{sec:results}
% Primary: Thomas & all

\subsection{LANL APEX Simulation Workflows on Cielo}

We consider the workload from LANL found in the APEX Workflows
report~\cite{apex2016} that consists of four applications
classes: EAP, LAP, Silverton and VPIC. The main characteristics of
these classes are reported in Table~\ref{table:lanl}. We simulate the
behavior of these applications over the Cielo Platform. Cielo is a
1.37 petaflops capability system installed in 2010-2011 at the Los
Alamos National Laboratory, with 143,104 cores, 286 TB of main memory,
and a parallel filesystem with a capacity of 160GB/s.

\begin{table}
\begin{tabular}{|l|c|c|c|c|}
\hline
 Workflow & EAP & LAP & Silverton & VPIC \\\hline
Workload percentage & 66 & 5.5 & 16.5 & 12 \\\hline
Work time (h) & 262.4 & 64 & 128 & 157.2 \\\hline
Number of cores & 16384 & 4096 & 32768 & 30000 \\\hline
Initial Input (\% of memory) &  3 & 5 & 70 & 10 \\\hline
Final Output (\% of memory) & 105 & 220 & 43 & 270 \\\hline
Checkpoint Size (\% of memory) & 160 & 185 & 350 & 85 \\\hline
\end{tabular}
\caption{LANL Workflow Workload from the APEX Workflows report.\label{table:lanl}}
\end{table}

Applications are randomly chosen to ensure that 1) the workload
percentage of each class is met within 1\% of its goal (row 2 of 
Table~\ref{table:lanl}); and 2) to
gather a simulated time of at least 2 months (62 days). To serve as
the baseline of comparison, we simulate the execution of each list of
applications without introducing checkpoints, faults, nor I/O
interference. From this simulation, we select a segment of the
execution that is 60 days long (excluding the beginning and end of the
simulation that present atypic behaviors), and compute the
resource used by the applications during that time. Since we consider
efficiency from a platform perspective and each application has a
different resource usage, we use the following metric to measure
resource usage: the total time each node spends doing a specific
operation (computation or I/O). Application usage sums the time spent
doing regular (non CR) I/O and computation that was not wasted by a
restart due to a failure.

For each checkpointing and I/O scheduling technique presented in
Section~\ref{sec:algorithms}, we then compute the resource waste, as
the total time spent either checkpointing or re-executing application
computation and I/O that were lost due to failures. In the figures, we
represent the performance of each strategy by computing the waste
ratio, i.e. the resource waste over a segment of 60 hours divided by
the application resource usage over that same segment for the baseline
simulation. Each simulation is conducted 2,000 times; the candlestick
extremes represent the first and last decile of the measures, while
the boxes represent the second and third quartile, and the point in
the middle is the mean value.

\begin{figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{sim/figures/synthetic-01hMTBF-waste-cielo.tex}}
  \end{center}
  \caption{Waste ratio as a function of the system bandwidth for the
    seven I/O and Checkpointing scheduling strategies, and the Cielo
    workload. \label{fig:cielo-1hmtbf}}
\end{figure}

First, we explore the performance of each approach under heavy risks
of failures. Figure~\ref{fig:cielo-1hmtbf} represents the waste ratio
on Cielo, assuming the node MTBF $\muind$ was 2 years (i.e. a system MTBF of
1h). We vary the filesystem bandwidth from 40 GB/s to 160GB/s in order
to evaluate the impact of this parameter. We observe three classes of
behavior: \propfixed and \bfifofixed exhibit a waste ratio that
decrease as the bandwidth increases, but remains above 40\% even with
a high available bandwidth; \fifodaly, \fifofixed, and \cooperative
quickly decrease at only 20\% of waste or less, and reach the
theoretical model performance; and \propdaly and \bfifodaly start at
the same level of efficiency as \propfixed and \bfifofixed, and reach
the 20\% of waste as the bandwidth increases.

This figure shows that with a high frequency of failures, providing
each application with the appropriate checkpoint interval is central
to relieve the filesystem from unnecessary (or even detrimental)
checkpoints, but this is not the sole criteria that should be taken
into account. The two strategies that remain with a high waste despite
a high bandwidth rely on a fixed 1h interval. As Figure~\ref{fig:cielo-1hmtbf} 
illustrates, simply relying on the Daly checkpointing period is not
sufficient to reach the best performance under constrained bandwidth:
at 60GB/s for the filesystem, \propdaly and \bfifodaly experience
twice the waste of the other strategies relying on the same value for
the checkpointing period. All strategies that decouple the execution
of the application from the filesystem availability (\fifodaly,
\fifofixed, \cooperative) exhibit much better performance despite low
bandwidth.

Notably, \cooperative remains the most efficient technique under all
conditions, and reaches the theoretical performance given by
Equation~\eqref{eq.totalwaste} for steady-state
analysis. This illustrates the efficiency of the proposed heuristic
(Equations~\eqref{eq.selection} and~\eqref{eq.selection2}) to
schedule checkpoints and I/O in a way that avoids interferences,
allowing the system to behave as if no interference was experienced,
in most cases. The high variation shows that a minority of the runs
experienced a significantly higher waste.

\begin{figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{sim/figures/synthetic-040gbs-waste-cielo.tex}}
  \end{center}
  \caption{Waste ratio as a function of the system MTBF for the
    seven I/O and Checkpointing scheduling strategies, and the Cielo
    workload. \label{fig:cielo-40gbs}}
\end{figure}

Second, we explore the performance of each approach under low
bandwidth (and thus high risk of
interference). Figure~\ref{fig:cielo-40gbs} represents the waste ratio
on Cielo, assuming the aggregated filesystem bandwidth of the system
was 40GB/s. We vary the node MTBF $\muind$ 
from 2 years (1h of system MTBF) to 50 years (24h of system MTBF) 
in order to evaluate the impact of this
parameter. Similarly to Figure~\ref{fig:cielo-1hmtbf}, we observe three
classes of behavior: \propfixed and \bfifofixed exhibit a waste ratio
that remains constant around 80\% for all values of the MTBF. These
approaches are critically dependent on the filesystem bandwidth, and a
lower frequency of failures does not significantly improve their
performance. The I/O subsystem is saturated, and the applications
spends most of their time waiting for it. \fifodaly, \fifofixed, and
\cooperative quickly fall at only 20\% of waste or less, and reach
the theoretical model performance; and \propdaly and \bfifodaly start
at the same level of efficiency as \propfixed and \bfifofixed, and
reach the 20\% of waste as the bandwidth increases.

For all the strategies that integrate the Daly checkpointing period
optimization, increasing the MTBF reduces the amount of I/O required
and thus enables to manage a constrained bandwidth easily. All
strategies that schedule the bandwidth are successful at increasing the
efficiency to values very close to the theoretical model. More
surprisingly, the simple FCFS scheduling of checkpoints with fixed
checkpoint interval (\bfifofixed) is capable of reaching a performance
comparable to the one of the strategies that reduce the number of
checkpoints. This is due to the significant reduction in interference
due to the reduction of number of restarts with a system MTBF of 8h
(16 years of node MTBF). As soon as the number of restarting
applications decreases, the filesystem sollicitation decreases
sufficiently to avoid long delays in non blocking approaches, as is
illustrated by the performance of \fifofixed at 2h of system MTBF (4
years of node MTBF).

\subsection{Prospective System}

\begin{figure}
  \begin{center}
    \resizebox{\linewidth}{!}{\input{sim/figures/prosp.tex}}
  \end{center}
  \caption{Minimum aggregated filesystem bandwidth to reach 80\%
    efficiency with the different approaches on the prospective
    future system.\label{fig:prosp}}
\end{figure}

In this section, we use the simulations to explore a prospective
system and define the impact of I/O and checkpoint scheduling when the
problem size and the machine size will increase. Based on the APEX
workflow report, we extrapolate the increase in problem size expected
for the application classes considered above, and project these
applications on a system that has the main characteristics of expected
future systems. We consider a future system with 7PB of main memory
and 50,000 nodes. We simulate the workload of Table~\ref{table:lanl}
on this machine, weak scaling the problem size proportionally to the
change in machine memory size. We consider different values of the
system MTBF; for each strategy, we find (by binary search) the
required system aggregated bandwidth necessary to observe 20\% of
waste (or 80\% of efficiency). The waste is computed, as previously,
by dividing the amount of resource used for checkpoints and lost due
to failures by the amount of resource used in a fault-free and
resilience-free run of the same initial conditions.

Figure~\ref{fig:prosp} shows the impact of the MTBF and of the
different strategies on this system. When failures are not a chronic
event, i.e. as soon as the node MTBF is at least 15 years
(corresponding to a system MTBF of 2.6h), a hierarchy of the different
approaches stabilizes: the two blocking strategies that rely on
frequent checkpoints (\propfixed and \bfifofixed) are the ones
requiring the highest bandwidth to reach 80\%
efficiency. % 6.4TB/s at 24 years
\fifofixed, that uses a fixed checkpoint interval comes next, with a
requirement around three quarter of the one required by \propfixed and
\fifofixed. It benefits from the capacity to continue working when the
filesystem is not available to checkpoint, which is sufficient when
failures are rare to obtain a significant performance
gain. % 4.9TB/s at 24 years
Then the four strategies that use the Daly optimal checkpointing
period, alleviating significantly the I/O pressure, are the ones that
require the lowest bandwidth to reach 80\% of efficiency, at about
half the required bandwidth of the current
situation. %coop: 2.9 TB/s at 24 years

When the failures are frequent (under 10 years of node MTBF), the
hierarchy changes as the critical element is solely to reduce  I/O
pressure: all strategies that use a fixed and frequent checkpoint
interval require a much higher system bandwidth to reach the 80\%
efficiency goal.  As noted before, the \cooperative approach is the
first to reach the bar in all cases. This is because it takes
advantage of the nonblocking approaches, integrates the Daly optimal
period optimization and schedules the I/O to reduce the waste.
This approach exhibits a strong resilience to failures, with a
bandwidth requirement that increases only by a factor 3 between the
very unstable simulations (at less than 1h of system MTBF), and the
most stable one (at 8h of system MTBF). Compared to this, the other
strategies are much more dependent upon the frequency of failures, with
the \propfixed strategy requiring up to 50 times the bandwidth of
\cooperative to reach the same efficiency in unstable conditions.
