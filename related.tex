
% !TEX root =  ipdps18.tex

\section{Related Work}\label{sec:related}
% Primary: Kurt

We first discuss research regarding checkpoint-induced I/O pressure, followed by
works that regard avoiding I/O interference.  These techniques are not necessarily
independent: generally, reducing I/O pressure will reduce the likelihood of
interference.  Therefore, we focus our I/O interference discussion to those
techniques which consider the global scheduling of checkpoints and/or application I/O
across a platform.

%\todo[inline]{kbf: I am unsure about this breakdown.  These two things do not
%seem independent; reducing pressure seems to al reduce interference ...}

\paragraph*{Checkpointing and I/O}

For a single application, the Young/Daly formula~\cite{young74,daly04} gives the
optimal checkpointing period. This period minimizes platform waste, defined as the
fraction of job execution time that does not contribute to its progress.  The two
sources of waste are the time spent taking checkpoints (which motivates longer
checkpoint periods) and the time needed to recover and re-execute after each failure
(which motivates shorter checkpoint periods). The Young/Daly period achieves the
optimal trade-off between these sources to minimize the total waste. Arunagiri et
al.~\cite{Arunagiri2010} studied longer, sub-optimal periods with the intent of
reducing I/O pressure and showed, both analytically and empircally using four real
platforms, that a decrease in the I/O requirement can be achieved with only a small
increase in waste.

\paragraph*{Reducing I/O Pressure}

There are two general strategies for reducing I/O pressure from a single application:
hiding or reducing checkpoint commit times without reducing checkpoint data volumes
and reducing commit times by reducing checkpoint data volumes.  Strategies that
attempt to hide checkpoint times include Diskless~\cite{Plank98Diskless} and remote
checkpoint protocols~\cite{Cornwell11RemoteBLCR} which leverage the typically higher
available bandwidths of the network or other storage media like RAM in order to
mitigate the performance of slower storage media like spinning or solid-state
disks. Additionally, remotely stored checkpoints have the additional benefit of
allowing systems to survive non-transient node failures. Similarly, multi-level
checkpoint protocols like SCR~\cite{Moody10SCR,Vaidya95TwoLevel} attempt to hide
checkpoint commit times by writing checkpoints to RAM, flash storage, or local disk
on the compute nodes~\cite{Kougkas2017} in addition to the parallel file system
thereby improving checkpoint or general I/O bandwidth.  Finally, checkpoint-specific
file systems like PLFS~\cite{Bent09PLFS} leverage the I/O patterns and
characteristics specific to checkpoint data to optimize checkpoint data transfers
to/from parallel file systems and therefore reduce checkpoint commit times.

Strategies that attempt to reduce checkpoint sizes include \emph{memory
exclusion}, which leverage user-directives or other hints to exclude portions of
process address spaces from checkpoints~\cite{Plank99MemoryExclusion}.
Additionally, incremental checkpointing protocols reduce checkpoint volumes by
utilizing the OS's memory page protection facilities to detect and save only
pages that have been updated between consecutive
checkpoints~\cite{Bronevetsky09Compiler,
Chen97CLIP,Elnozahy92ConsistentCheckpointing,Li94ConcurrentCheckpointing,
Plank94Libckpt,Paun10IncrementalWeibull,Kiswany08stdchk}.  Similarly,
page-based hashing techniques can also be used to avoid checkpointing pages
that have been written to but whose content has not
changed~\cite{Ferreira11Libhashckpt}.  Finally, compression-based techniques
use standard compression algorithms to reduce checkpoint
volumes~\cite{Ibtesham12Compression} and can be used at the
compiler-level~\cite{Li90CATCH} or in-memory~\cite{Plank94ICKP}.  Related,
Plank et al. proposed \textit{differential compression} to reduce checkpoint
sizes for incremental checkpoints~\cite{Plank95CompressedDiff} and Tanzima et
al.  show that similarities amongst checkpoint data from different processes
can be exploited to compress and reduce checkpoint data
volumes~\cite{tanzima12mcrengine}.  Finally, Sasaki et al propose a lossy
compression method based on wavelet transform and vector quantization to the
checkpoints of a production climate application~\cite{sasaki2015}, while Ni et
al~\cite{Ni2014} study the trade-offs between the loss of precision, compression
ratio, and application correctness due to lossy compression.\dca{Can we say the
  outcomes from these latter studies?}
\kbf{Yes, but why???  This is not a lossy compression paper}

\paragraph*{Avoiding I/O interference}

Most closely related to our work, a number of studies have considered the global
scheduling of checkpoints and other I/O across a platform to reduce overall
congestion, thereby increasing performance.  Aupy et al.~\cite{Aupy:2017:Periodic}
presented a decentralized I/O scheduling technique for minimizing the congestion due
to checkpoint interference by taking advantage of the observed periodic and
deterministic nature of HPC application checkpoints and I/O.  This technique allows
the job scheduler to pre-define each applicationâ€™s I/O behavior for their entire
execution.  Similarly, a number of works have investigated the efficiency of online
schedulers for data intensive~\cite{Groot2013,Sim:2015:AnalyzeThis} and HPC workload
I/O~\cite{Dorier2015,Gainaru:2016:Scheduling,Zhou:2015:IOAware,Herbein2017}.
Finally, a number of works have investigated utilizing recorded system reliability
information~\cite{Oliner:2006:Cooperative} and the statistical properties of these
failures~\cite{Tiwari:2014:Lazy} to determine effective checkpoint intervals for the
portion of the system used by the workload.

\paragraph*{Summary}

We distinguishes our work from these previous studies in a number of important ways.
First, unlike a number of the previous studies, our technique considers existing
non-CR application I/O. Additionally, our approach is agnostic to the I/O patterns of
the considered applications as long as they are known.  Also, we attempt to optimize
the efficiency of the entire platform, with the changing workloads and failures
running on that platform, rather than just considering one workload. Finally and most
importantly, this approach provides optimal checkpointing periods in environments
where I/O is highly constrained and Daly/Young's formula is less appropriate, a common
scenario on many leadership-class systems.
