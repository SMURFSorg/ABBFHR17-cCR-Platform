% !TEX root =  ipdps18.tex

\section{Simulation Framework}
\label{sec:simulator}
% Primary: Thomas

We used discrete event simulations to evaluate the performance of the proposed
approaches.  Our simulations\footnote{The simulator is publicly
  available on~\url{https://github.com/SMURFSorg/InterferingCheckpoints}.} are
instantiated by a set of initial conditions that define a set of application classes,
the distribution of resource usage between application classes, and the main
characteristics of the platform on which application instances will execute.

\paragraph*{High level parameters}
Application classes are characterized by: initial input and output sizes, checkpoint
size, quantity of work to execute, number of nodes to use, volume of diffuse I/O to
execute during job makespan, and execution time. \dca{what does diffuse I/O mean?}
\dca{what is the difference between execution time and ``quantity of work to
  execute?''}  Platforms are characterized by the number of nodes, a system Mean Time
Between Failures, and an aggregated I/O subsystem bandwidth that is shared among the
nodes. We assume symmetric read and write filesystem bandwidths, hence
$\ckpt{i}=\reco{i}$ for each application class, $\app{i}$.

A simulation first selects randomly a list of jobs that are instances of the
different application classes. This list is ordered by job priority, and constrained
by two parameters: the minimum simulated time to consider, and the relative
proportion of platform resources used by each application class (based on the APEX
report). \dca{I don't recall any prior discussion of job priority.}  As an example,
we consider the subset of application classes given by the APEX workflows report for
the subset of application classes of LANL (EAP, LAP, Silverton and VPIC), simulated
as running over the Cielo supercomputer, for a minimal execution time of 60h. A
simulation will randomly instantiate one of the four classes, assigning a work
duration uniformly distributed between $0.8w$ and $1.2w$, where $w$ is the typical
walltime specified for the chosen application class, and count the resource allocated
for this application class, until A) the simulated execution would necessarily run
for at least 2 months, and B) the amount of resource used by the selected class is
within 1\% of the target goal of the representative workload percentage defined in
the APEX workflows report (see Table~\ref{table:lanl}).

In addition to the jobs list, we generate a set of node failure times according to an
exponential distribution with the specified MTBF. At the chosen times, we randomly
choose which of the system nodes fails.  The jobs list and failure times constitute
the initial conditions of a simulation.

\paragraph*{Job Scheduling}
We compute a job schedule (start and end times for all jobs in the list) using a
simple first-fit strategy and considering job characteristics, job priority and
resource availability.  We simulate online scheduling: whenever a job prematuraly
ends (because of a failure or because it reached the end of its execution before the
planned time), the schedule is amended by re-scheduling all jobs that were not
started yet. \dca{How does a job end before its planned end time?}

\paragraph*{Execution Simulation}
Once a job is started, it first executes its initial input. It then 1) executes some
work for a certain period before it 2) checkpoints. These two steps are repeated
until all planned work is executed, after which the final output is executed by the
job, before it ends. At any time during the execution, a node hosting the job may be
subject to a failure (according to the pre-computed failure times and places). When
that happens, the job is terminated, and a new job is added to the list of jobs to
schedule. That new job represents the restart of the failed one: it has similar
characteristics except its initial input corresponds to the restart size, and its
work time corresponds to the remaining work from the last successful checkpoint. To
reflect a common job scheduling policy on shared platforms, restarted jobs are set to
the highest priority, maximizing their higher chances of obtaining an immediate
allocation and continuing what was the original (failed) jobs execution.

\dca{Moved the discussion of fixed v. Daly to model section. Don't know what we want
to do with the remaining snippet since it didn't quite fit there: ``For the
\leastwaste algorithm, the checkpoint interval is at least the Daly period
(by construction), and a Fixed version is pointless.''}

\ifTR

\paragraph*{Interference Models} Simulations implement each of the
interference model and avoidance strategies defined in
Section~\ref{sec:algorithms}: for \propfixed and \propdaly,
interfering I/O and checkpoints get a portion of the available
aggregated bandwidth proportional to the number of nodes they use, and
inversely proportional to the number of nodes involved for all
jobs doing I/O; for \bfifofixed and \bfifodaly, I/O requests
and checkpoints are ordered in a first-come first-serve strategy, and
when they are selected, obtain the full bandwidth; for \fifofixed and
\fifodaly, I/O requests and checkpoints are served in order, but the
simulation adds all the time waiting for a checkpoint to start as
progress in the computation for the job; and for \cooperative,
the same is implemented, but I/O are ordered to minimize the waste in
Equations~\eqref{eq.selection} and~\eqref{eq.selection2}.


Note that in the scheduled I/O methods (\fifononblock and \cooperative),
initial inputs and final outputs are blocking (the job
cannot progress during the I/O until it is served), but checkpoints
are non blocking, which entails that if a failure hits the job,
it may have to re-execute from a checkpoint far in its past, if it was not
granted access to the filesystem for a long time.
\else
Simulations implement each of the interference models and avoidance
strategies defined in Section~\ref{sec:algorithms}.
\fi

\paragraph*{Method of statistics collection from simulations}
We run each simulation once per strategy with the same initial conditions (list of
jobs with initial priorities and date and location of failures). \dca{Why only once
  since we can generate different random scenarios that map to the same
  characteristics? I can't rationalize this statement with the one below that says we
  simulate thousands ...}  We collect performance statistics for each simulation.  In
order to consolidate these measures over many simulations, statistics are taken over
a segment of fixed length of each simulation. \dca{I don't understand the
  ``consolidation'' concept.}  To capture steady-state, this segment excludes both
the first day of the simulation (in which jobs might be synchronized artificially
because a subset starts at the same date), and the last day of the simulation (in
which a large amount of resource may not be used as new jobs are not added to the
workload). We simulate a large number of such runs (at least a thousand simulations
per shown measurement), and compute the first, and ninth decile and the first and
third quartile of the performance for each statistic, as well as the mean value.
