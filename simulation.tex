% !TEX root =  ipdps18.tex

\section{Simulation Framework}
\label{sec:simulator}
% Primary: Thomas

In order to evaluate the performance of the proposed approach, we ran
a large set of discrete event simulations that we describe in this
section. Simulations are instantiated by a set of initial conditions
that define a set of application classes, their distribution, and on
the main characteristics of the platform on which these will execute.

\paragraph*{High level parameters}
Application classes are characterized by the following parameters:
size of initial input and output, size of checkpoints, quantity of
work to execute, number of nodes to use, quantity of diffuse I/O to
execute during the entire life of the application, and execution time.
Platforms are characterized by the number of nodes, a system Mean Time
Between Failures, and an aggregated I/O subsystem bandwidth that is
shared between the different nodes. We assume same bandwidth when
reading from and writing to the filesystem, hence $\ckpt{i}=\reco{i}$
for each application $\app{i}$.

A simulation first selects randomly a list (ordered) of applications
that are instances of the different application classes. This list is
constrained by two parameters: the minimum simulated time to consider,
and the relative proportion of platform resources allocated to each
application class.

As an example, we consider the subset of application classes given by
the APEX workflows report for the subset of application classes of
LANL (EAP, LAP, Silverton and VPIC), simulated as running over the
Cielo supercomputer, for a minimal execution time of 60h. A simulation
will randomly instantiate one of the four classes, assigning a work
duration uniformly distributed between $0.8w$ and $1.2w$, where $w$ is
the typical walltime specified for the chosen application class, and
count the resource allocated for this application class, until A) the
simulated execution would necessarily run for at least 2 months, and B) the
amount of resource used by the selected class is within 1\% of the
target goal of the representative workload percentage defined in the
APEX workflows report (see Table~\ref{table:lanl}).

Once this list of applications is defined, a set of node failures
dates is computed. Dates of failures are placed following an
exponential distribution with the MTBF required for the simulation. At
the chosen dates, which node is hit by the failure is chosen uniformly
between all nodes of the simulated platform. The (ordered) list of
applications and the date and location of failures constitute the
initial conditions of a simulation.

\paragraph*{Job Scheduling}
Then, an initial job schedule is computed: applications are set to
start and end at planned dates and on planned nodes, depending on
their characteristics, the job schedule and the order they appear in
the ordered set. The job schedule follows a simple first-fit
strategy. We simulate an online scheduling, and every time an
application ends before its planned date (because of a failure, or
because it reached the end of its execution before the planned date),
the schedule is amended by de-scheduling all applications that were
not started yet, and re-scheduling them following the current order of
applications.

\paragraph*{Execution Simulation}
Once an application is started, it first executes its initial
input. It then executes some work for a certain period and it
checkpoints. These two steps are repeated until all planned work is
executed, after which the final output is executed by the application,
before it ends. At any time during the execution, a node hosting the
application may be subject to a failure (striking at pre-computed
dates and places). When that happens, this application is terminated,
and a new application is added to the list of applications to
schedule. That new application represents the restart of the failed
application: it has similar characteristics as the failed application,
but an initial input corresponding to the restart size, and a work
time corresponding to what was remaining to do at the date of last
successful checkpoint. To reflect a frequent policy on shared
platforms, this restarting application is pushed in front of the
ordered list of applications to execute, so that it gets a higher
priority to obtain resources and complete its execution.

Checkpointing periods can either be dynamic or fixed. Many users
define a fixed period based on their own experience on the
platform. An ordinary heuristic is to take a checkpoint every hour,
with the reasoning that in the worst case, only one hour of work can
be lost. We also consider a dynamic period that is defined for each
application. We use for this period the Young/Daly
approximation~\cite{young74,daly04}, where the checkpoint time is
taken as the time it would take to checkpoint the application if there
were no interference.

\paragraph*{Interference Models} Simulations implement each of the
interference model and avoidance strategies defined in
Section~\ref{sec:algorithms}: for \propfixed and \propdaly,
interfering I/O and checkpoints get a portion of the available
aggregated bandwidth proportional to the number of nodes they use, and
inversely proportional to the number of nodes involved for all
applications doing I/O; for \bfifofixed and \bfifodaly, I/O requests
and checkpoints are ordered in a first-come first-serve strategy, and
when they are selected, obtain the full bandwidth; for \fifofixed and
\fifodaly, I/O requests and checkpoints are served in order, but the
simulation accounts all the time waiting for a checkpoint to start as
progress in the computation for the application; and for \cooperative,
the same is implemented, but allowing the order to be reconsidered
according to Equations~\ref{eq.selection} and~\ref{eq.selection2}.
Note that this means that if a failure hits the application, it may
have to re-execute from a checkpoint far in its past, if it was not
granted access to the filesystem for a long time.

\paragraph*{Method of statistics collection from simulations}
As we compare all the scheduling strategies, each simulation is run
once per strategy with the same initial conditions (ordered list of
applications and date and location of failures). Performance
statistics are collected over each simulation. In order to consolidate
these measures over many simulations, statistics are taken over a
segment of fixed length of each simulation. This segment excludes both
the first day of the simulation (in which applications might be
synchronized artificially because a subset starts at the same date),
and the last day of the simulation (in which a large amount of
resource may not be used as new applications are not added to the
workload). We simulate a large number of such runs (at least a
thousand simulations per shown measurement), and compute the first,
second, third quartile of the performance for each statistic, as well
as the mean value.
