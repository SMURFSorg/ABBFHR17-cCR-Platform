% !TEX root =  ipdps18.tex

\section{Simulation Framework}
\label{sec:simulator}
% Primary: Thomas

We use discrete event simulations to evaluate the performance of the proposed
approaches.  Our simulations\footnote{The simulator is publicly available
from~\url{https://github.com/SMURFSorg/InterferingCheckpoints}.} are instantiated
by a set of initial conditions that define a set of application classes, the
distribution of resource usage between application classes, and the main
characteristics of the platform on which application instances will execute.

\paragraph*{High level parameters}
Application classes are characterized by: initial input and output sizes, checkpoint
size, quantity of work to execute, number of nodes to use, volume of I/O to
execute during job makespan, and execution time.
% \dca{what does diffuse I/O mean?}
% TH Removed confusing adjective 
%\dca{what is the difference between execution time and ``quantity of
%work to execute?''} 
% TH: for me, execution time implies from start to finish, and that
% includes time to do I/O and checkpoints. Quantity of work to execute
% implies how long to compute, and that is constant whatever the
% duration of I/O or checkpoints.
% Obviously, execution time > time to do the quantity of work, and
% more importantly, execution time depends on the strategy used to
% checkpoint and schedule the I/Os, so we need something constant to
% define the workload.
Platforms are characterized by the number of nodes, a system Mean Time
Between Failures, and an aggregated I/O subsystem bandwidth that is shared among the
nodes. For simplicity, we assume symmetric read and write filesystem bandwidths, hence
$\ckpt{i}=\reco{i}$ for each application class, $\app{i}$.

A simulation first randomly selects a list of jobs that are instances of the
different application classes. This list is ordered by job priority, and constrained
by two parameters: the minimum simulated time to consider, and the relative
proportion of platform resources used by each application class (based on the APEX
report~\cite{apex2016}). 
%\dca{I don't recall any prior discussion of job priority.}  
% TH: There is an implicit order in first fit scheduling. That
% priority makes it explicit. It's important because we re-schedule,
% it's in particular important when failures happen to understand in
% which order the apps are considered.
As an example,
we consider the subset of application classes given by the APEX workflows report for
the subset of application classes of LANL (EAP, LAP, Silverton and VPIC), simulated
as is executed on the Cielo supercomputer, for a minimal execution time
of 60 day. A
simulation will randomly instantiate one of the four classes, assigning a work
duration uniformly distributed between $0.8w$ and $1.2w$, where $w$ is the typical
walltime specified for the chosen application class, and count the resource allocated
for this application class, until 1.)~the simulated execution would necessarily run
for at least 2 months, and 2.)~resources used by the selected class is
within 1\% of the target goal of the representative workload percentage defined in
the APEX workflows report (see Table~\ref{table:lanl}).

In addition to the jobs list, we generate a set of node failure times according to an
exponential distribution with the specified MTBF. At the chosen times, we randomly
choose which of the nodes fail.  These jobs list and failure times constitute
the initial conditions of a simulation.

\paragraph*{Job Scheduling}

We compute a job schedule (start and end times for all jobs in the list) using
a simple first-fit strategy considering: job characteristics, job priority and
resource availability.  We simulate online scheduling; whenever a job
prematurely ends, whether due to failure or because it reached the end of its
execution before the planned time, the schedule is amended by re-scheduling all
jobs that were not started yet.
% \dca{How does a job end before its planned end time?} TH: well, it can, but
% this can be confusing... Here is the full story: - we set a planned end date
% based on estimates of duration of checkpoints, number of checkpoints and I/O
% - but these estimates cannot be right all the times: checkpoints and I/Os
%   will be delayed - So when a job spans over its end date, we extend its
%   duration by re-estimating what remains to do (work, I/O, checkpoints) -
%   When we extend a job duration we re-schedule jobs that did not start yet
%   - After a while, the job finally completes, and it can happen it completes
%   before the estimate (because of rounding errors and/or over-estimation of
%   checkpoint / I/O duration If you want to explain this story here, feel free
%   to do so. I thought this was going to be a lot of details and make the
%   reader focus on elements that are not central to the result.

\paragraph*{Execution Simulation}

Once a job is started, it executes its initial input. It then, 1.)~executes
some work for a certain period before it, and 2.)~checkpoints. These two steps
are repeated until all planned work is executed, after which the final output
is executed by the job, before it ends.  At any time during the execution, a
node hosting the job may be subject to a failure (according to the pre-computed
failure times and location). When that happens, the job is terminated and a new
job is added to the list of jobs to schedule. That new job represents the
restart of the failed one; it has similar characteristics except its initial
input corresponds to the restart size, and its work time corresponds to the
remaining work from the last successful checkpoint. To reflect a common job
scheduling policy on shared platforms, restarted jobs are set to the highest
priority, maximizing their chances of obtaining an immediate allocation and
continuing what was the original (failed) jobs execution.

%\dca{Moved the discussion of fixed v. Daly to model section. Don't know what we want
%to do with the remaining snippet since it didn't quite fit there: ``For the
%\leastwaste algorithm, the checkpoint interval is at least the Daly period
%(by construction), and a Fixed version is pointless.''}
% TH: Yves put a footnote in III.E about that, so we can ditch the
% paragraph entirely

\ifTR

\paragraph*{Interference Models} Our simulations implement each of the
interference models and avoidance strategies defined in
Section~\ref{sec:algorithms}: for \propfixed and \propdaly,
interfering I/O and checkpoints get a portion of the available
aggregated bandwidth proportional to the number of nodes they use, and
inversely proportional to the number of nodes involved for all
jobs doing I/O; for \bfifofixed and \bfifodaly, I/O requests
and checkpoints are ordered in a first-come first-served basis, and
when they are selected, obtain the full bandwidth; for \fifofixed and
\fifodaly, I/O requests and checkpoints are served in order, but the
simulation adds all the time waiting for a checkpoint to start as
progress in the computation for the job; and for \cooperative,
the same is implemented, but I/O is ordered to minimize the waste in
Equations~\eqref{eq.selection} and~\eqref{eq.selection2}.

Note that in the scheduled I/O methods (\fifononblock and \cooperative),
initial inputs and final outputs are blocking (the job cannot progress during
the I/O until it is served), but checkpoints are non-blocking, which means that
if a failure hits the job, it may have to re-execute from a checkpoint far in
its past if it has  not been granted access to the filesystem for an extended
period of time.

\else

Simulations implement each of the interference models and avoidance
strategies defined in Section~\ref{sec:algorithms}.
\fi

\paragraph*{Method of statistics collection from simulations}
Each simulation runs the same initial conditions (list of jobs with
initial priorities and date and location of failures) on each
strategy.
% \dca{Why only once since we can generate different random scenarios
% that map to the same characteristics? I can't rationalize this
% statement with the one below that says we simulate thousands ...}
% TH: I obviously said that in a confusing way. I have updated it,
% feel free to improve the wording :) What I meant is that each
% strategy is run on the same initial conditions, we extract the
% measure from each simulation, and compute ratios. If the initial
% conditions were changing between runs, it would take even more runs
% to reduce variability.
We collect performance statistics over a segment of fixed length of
each simulation.
%\dca{I don't understand the ``consolidation'' concept.}  
%TH me neither.  I have removed it.
% I feel we lost a discussion here why we need to do so.
% We were explaining before that we cannot simply average the time
% spent doing useful work or wasting resource between two set of
% simulations (two set with different initial conditions), because
% different initial conditions imply different work load, different
% number of applications, etc. So, the idea was to use a segment of
% fixed length and extract work ratio / waste ratio from these. Since
% they have the same denominator, simple average still makes sense.
To capture steady-state, this segment excludes both the first day of
the simulation (in which jobs might be synchronized artificially
because a subset starts at the same date), and the last day of the
simulation (in which a large amount of resource may not be used as new
jobs are not added to the workload). We simulate a large number of
such runs (at least a thousand simulations per shown measurement), and
compute the first, and ninth decile and the first and third quartile
of the performance for each statistic, as well as the mean value.
