% !TEX root =  ipdps18.tex

\section{Simulation Framework}
\label{sec:simulator}
% Primary: Thomas

We use discrete event simulations to evaluate the performance of the proposed
approaches.  Our simulations\footnote{The simulator is publicly available
from~\url{https://github.com/SMURFSorg/InterferingCheckpoints}.} are instantiated
by a set of initial conditions that define a set of application classes, the
distribution of resource usage between application classes, and the main
characteristics of the platform on which application instances will execute.

\paragraph*{High level parameters}
Application classes are characterized by: initial input and output sizes, checkpoint
size, quantity of work to execute, number of nodes to use, volume of I/O to
execute during job makespan, and execution time.

% TH: for me, execution time implies from start to finish, and that
% includes time to do I/O and checkpoints. Quantity of work to execute
% implies how long to compute, and that is constant whatever the
% duration of I/O or checkpoints.
% Obviously, execution time > time to do the quantity of work, and
% more importantly, execution time depends on the strategy used to
% checkpoint and schedule the I/Os, so we need something constant to
% define the workload.
%
%DA: How about "job work volume" vs "job makespan": the former is the
%constant workload and the latter is the time to execute that load
%given all the system's non-determinism.

Platforms are characterized by the number of nodes, a system Mean Time
Between Failures, and an aggregated I/O subsystem bandwidth that is shared among the
nodes. For simplicity, we assume symmetric read and write filesystem bandwidths, hence
$\ckpt{i}=\reco{i}$ for each application class, $\app{i}$.

A simulation first randomly selects a list of jobs that are instances
of the different application classes. This list is ordered by job
priority (\ie, arrival time for our FCFS algorithms) and constrained
by two parameters: the minimum simulated time to consider, and the
relative proportion of platform resources used by each application
class (based on the APEX report~\cite{apex2016}).  As an example, we
consider the subset of application classes given by the APEX workflows
report for the subset of application classes of LANL (EAP, LAP,
Silverton and VPIC), simulated as is executed on the Cielo
supercomputer, for a minimal execution time of 60 day. A simulation
will randomly instantiate one of the four classes, assigning a work
duration uniformly distributed between $0.8w$ and $1.2w$, where $w$ is
the typical walltime specified for the chosen application class, and
count the resource allocated for this application class, until 1.)~the
simulated execution would necessarily run for at least 2 months, and
2.)~resources used by the selected class is within 1\% of the target
goal of the representative workload percentage defined in the APEX
workflows report (see Table~\ref{table:lanl}).

In addition to the jobs list, we generate a set of node failure times according to an
exponential distribution with the specified MTBF. At the chosen times, we randomly
choose which of the nodes fail.  These jobs list and failure times constitute
the initial conditions of a simulation.

\paragraph*{Job Scheduling}

We compute a job schedule (start and end times for all jobs in the list) using
a simple first-fit strategy considering: job characteristics, job priority and
resource availability.  We simulate online scheduling; whenever a job
prematurely ends, whether due to failure or because the job reached the end of its
execution before the planned time, the schedule is amended by re-scheduling all
jobs that were not started yet.
% \dca{How does a job end before its planned end time?} TH: well, it can, but
% this can be confusing... Here is the full story: - we set a planned end date
% based on estimates of duration of checkpoints, number of checkpoints and I/O
% - but these estimates cannot be right all the times: checkpoints and I/Os
%   will be delayed - So when a job spans over its end date, we extend its
%   duration by re-estimating what remains to do (work, I/O, checkpoints) -
%   When we extend a job duration we re-schedule jobs that did not start yet
%   - After a while, the job finally completes, and it can happen it completes
%   before the estimate (because of rounding errors and/or over-estimation of
%   checkpoint / I/O duration If you want to explain this story here, feel free
%   to do so. I thought this was going to be a lot of details and make the
%   reader focus on elements that are not central to the result.
%
%   DCA: My (perhaps incorrect) understanding of how we compute a
%   job's end time is by considering that job's start time, volume of
%   work, and I/O (normal and CR). Since this computation doesn't
%   factor in failures or interference, this seems to render a
%   makespan lower bound. Those other factors would only increase (not
%   decrease) makespan. I think I may be still missing something.

\paragraph*{Execution Simulation}

Once a job is started, it executes its initial input. It then, 1.)~executes
some work for a certain period before it, and 2.)~checkpoints. These two steps
are repeated until all planned work is executed, after which the final output
is executed by the job, before it ends.  At any time during the execution, a
node hosting the job may be subject to a failure (according to the pre-computed
failure times and location). When that happens, the job is terminated and a new
job is added to the list of jobs to schedule. That new job represents the
restart of the failed one; it has similar characteristics except its initial
input corresponds to the restart size, and its work time corresponds to the
remaining work from the last successful checkpoint. To reflect a common job
scheduling policy on shared platforms, restarted jobs are set to the highest
priority, maximizing their chances of obtaining an immediate allocation and
continuing what was the original (failed) jobs execution.


\ifTR

\paragraph*{Interference Models} Our simulations implement each of the
interference models and avoidance strategies defined in
Section~\ref{sec:algorithms}: for \propfixed and \propdaly,
interfering I/O and checkpoints get a portion of the available
aggregated bandwidth proportional to the number of nodes they use, and
inversely proportional to the number of nodes involved for all
jobs doing I/O; for \bfifofixed and \bfifodaly, I/O requests
and checkpoints are ordered in a first-come first-served basis, and
when they are selected, obtain the full bandwidth; for \fifofixed and
\fifodaly, I/O requests and checkpoints are served in order, but the
simulation adds all the time waiting for a checkpoint to start as
progress in the computation for the job; and for \cooperative,
the same is implemented, but I/O is ordered to minimize the waste in
Equations~\eqref{eq.selection} and~\eqref{eq.selection2}.

Note that in the scheduled I/O methods (\fifononblock and \cooperative),
initial inputs and final outputs are blocking (the job cannot progress during
the I/O until it is served), but checkpoints are non-blocking, which means that
if a failure hits the job, it may have to re-execute from a checkpoint far in
its past if it has  not been granted access to the filesystem for an extended
period of time.

\else

Simulations implement each of the interference models and avoidance
strategies defined in Section~\ref{sec:algorithms}.
\fi

\paragraph*{Method of statistics collection from simulations}
For a given scheduling strategy, all simulated runs begin with the
same initial conditions (jobs list, job priorities and failure
instances).  We collect performance statistics over a fixed length
segment of each simulation.  To capture steady-state behaviors, this
segment excludes the first and last days of the simulation. (During
the first day, jobs may be synchronized artificially because a subset
starts at the same date, and during the last day, large amounts of
resources may not be used as new jobs are not added to the workload).
Each measurement given in Section~\ref{sec:results} aggregates the
observations of at least a thousand simulations: for each aggregate
measurement, we compute and show mean, first and ninth decile, and
first and third quartile statistics.  Since simulations for the
various scheduling strategies have different initial conditions
(including job mix), it would be misleading to compare simple averages
of the time spent doing useful work (or time wasted) across simulation
instances. Instead, we use the fixed-length, steady-state segments
across simulation instances to extract and compare waste/work ratios
that can be compared appropriately.

% we cannot simply average the time
% spent doing useful work or wasting resource between two set of
% simulations (two set with different initial conditions), because
% different initial conditions imply different work load, different
% number of applications, etc. So, the idea was to use a segment of
% fixed length and extract work ratio / waste ratio from these. Since
% they have the same denominator, simple average still makes sense.

% If the initial conditions were changing between runs, it would take
% even more runs to reduce variability.
%
% I feel we lost a discussion here why we need to do so.
% We were explaining before that we cannot simply average the time
% spent doing useful work or wasting resource between two set of
% simulations (two set with different initial conditions), because
% different initial conditions imply different work load, different
% number of applications, etc. So, the idea was to use a segment of
% fixed length and extract work ratio / waste ratio from these. Since
% they have the same denominator, simple average still makes sense.


