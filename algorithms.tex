\section{I/O Scheduling Algorithms}\label{sec:algorithms}
% Aurelien & George
\dca{I think this section currently conflates general concepts with specific
  algorithmic details. Whenever we believe the content is close to fixed, I'd like to
  take a pass at refactoring the discussion.}

  %TODO: again this is part of the model
  %TODO: looks like I'm not using the right terminology for blocking/non-blocking/sharing...
  \subsection{Dealing with interference for non-cooperative application scheduling}
  \begin{itemize}
    \item For coherence, we use the same policy for input I/O, output I/O and checkpoint operations \todo[inline]{we may want to separate Input+recovery from output+checkpoint (bidirectional channels)}
    \todo[inline]{answer: we could but not sure it is 100\% independent; and it would complicate things without changing the story.}
    \item Blocking: only one application can perform I/O operations at the given time-step.
    I/O requests are served on a FCFS basis. When an application has requested an I/O
    operation, it waits idle until being served. \dca{Technically, blocking is
      cooperative!}
    \item Sharing: the I/O resource is shared at every time-step to serve all concurrent requests. Sharing is fair, i.e., bandwidth is inversely proportional to number of requests.
    \item Here is an example:
    application $A_{1}$ wants to perform five gigabytes of I/O at time $t$,
    and available bandwidth is
    $1$ gigabyte per second. If there is no other request, the operation will take five seconds.
    Now application $A_{2}$ (with same number of processors as $A_{1}$, hence with same assigned bandwidth in the sharing policy) wants to perform two gigabytes of I/O at time $t+1$.
    \begin{itemize}
    \item Blocking: $A_{2}$'s operation is delayed until time $t+5$ and will complete at $t+7$.
    \item Sharing: Instead of taking $2$ seconds, $A_{2}$'s operation will take $4$ seconds, because the bandwidth is shared with $A_{1}$ during its whole transfer. As a results, $A_{2}$'s operation completes at $t+5$ while $A_{1}$'s  operation is slowed down during four units and now completes at $t+7$.
    \end{itemize}
  \end{itemize}

In this section, we present the algorithms used to schedule applications
and I/O workload in order to alleviate the effect of concurrent access
to I/O resources. The first algorithm (\nocoop) represents the status-quo  in
which applications are scheduled in a non-cooperative manner, which may
incur interference on I/O resource access and wait time. The second
algorithm (\fifoblock) inserts some coordination to eliminate interference
between I/O activities. Only one application performs I/O at any given
time, other applications are blocked in their
I/O routines. When the active application finishes its I/O, the head of
the line application is granted the checkpointing token and can proceed.
The third algorithm (\fifononblock) is similar, except that
applications that are waiting for the checkpointing token do not block
and may continue computing until their turn comes; note that unlike the
non-cooperative and blocking algorithsm, this optimization requires
application code refactoring. Last, we propose the an heuristic
(\leastwaste) that improves on \fifononblock by selecting,
the application that inflicts the least overhead on the system:
Instead of following a FIFO order to select the next I/O application,
for each requesting application, the heuristic computes the prospective
waste incurred by delaying its I/O (considering checkpoint and
probabilistic recovery costs, idle time, etc.) when selecting another
application, and selects the one that minimizes the waste increase
at the current instant.

\subsection{Non-cooperative I/O Scheduling}

In the non-cooperative I/O scheduling \nocoop, applications are selected to
fill-up the system based on processor count availability. The I/O
workload (including checkpointing activities) are not organized by any
comprehensive system. Instead, it is assumed that concurrent access
to I/O resources will cause a decrease in the application observed
bandwidth for I/O operations. One can imagine multiple cost functions
for the effect of that sharing; When the filesystem is scalable, the
overall throughput of the platform should be maintained when multiple
applications concurently access, and each application should thereby
observe a linear decrease in its own bandwidth. As the application is
blocking on the completion of its I/O operations before it can continue,
that decrease in observed bandwidth leads to a proportionate increase in
I/O time that must be accounted as interference generated waste.

\subsection{Blocking FIFO I/O Scheduling}

A simple optimization to the aforementionned scheme is to favor one of
the applications' I/O request over all others. While the overall throughput
may remain unchanged (given an efficient PFS implementation), the favored
application completes its I/O workload faster (\ie at nominal speed).
Applications are favored in the order of their I/O requests, \ie
as soon as an application starts blocking on an I/O operation it will
take its place in the back of a FIFO queue (\fifoblock).

The advantage can be trivially seen in a simple workload with two
applications and assuming a favorable linear interference model.
If the two application start, simultaneously, an I/O requesting the
transer of a similar volume $V$ of data, in the \nocoop strategy,
both applications take $V*2/\bandtotal$ time to complete their I/O.
In the \fifoblock strategy, the first (as serialized when inserting in
the FIFO) application takes $V/\bandtotal$, while the second application
waits $V/\bandtotal$ before the I/O starts, but then enjoys the full use of
the I/O system and therefore still completes in $V*2/\bandtotal$. Thanks
to reducing I/O interferences, the average completion time of the I/O
has been reduced for the applications (altough fairness has been decreased).

\subsection{Non-Blocking FIFO I/O Scheduling}

In the previous strategy, the cost of I/O interferences has been
exchanged for idle time when waiting for the I/O token in a blocking
fashion. If the application programmer can refactor his code in order
to continue computing while the I/O request is waiting in queue,
then, it is possile to overlap that idle time with useful computation.
While it may sometimes be difficult to overlap initial and final input
and output, checkpointing I/O workload can be effectively time-shifted.
Note that in coordinated checkpoints strategies, each individual node
checkpoint is part of a set forming a coherent restart point for the
whole application, and the state must be captured precisely at the
requesting date for the global checkpoint to be correct. However, that
 state can be initially captured by copy-on-write mechanisms, or stored
in local memory or in compute node-local burst buffers (\eg local SSD
drives). Although node-local burst buffers do not offer protection
against faults, they permit offsetting the transfer of the checkpoint
data to a later date when the I/O token is available to the application.
When the application finaly gets the token, the previously scratch-space
stored checkpoint is transfered to the PFS without interference.
%TODO: something about replacing with last ckpt if token doesn't come in fast enough

\subsection{Least-waste algorithm}

The \leastwaste algorithm further refines on the \fifononblock algorithm
by giving the I/O token to the application that generates the least
waste, rather than simply in requesting order. Note that given the time
dependent nature of that descision, the selection may
not be the global optimum, but only an approximation given currently
available information about the system status.



\subsubsection{Strategy}

\begin{itemize}
  \item We always use the blocking strategy but not the FCFS policy.
  \item Instead, whenever an I/O operation completes at time $t$, we have a pool of application candidates:
  \begin{itemize}
   \item Category \IOcat $\Catiocat$: Applications $A_{i}$, $1\leq i \leq r$, which need to do input I/O, output I/O or recovery
  \item Category \Ckptcat $\Catckptcat$: Applications $A_{i}$, $r+1\leq i \leq r+s$,
  whose last checkpoint took place no later than time $t - \period{Daly}(A_{i})$, where $\period{Daly}(A_{i})$ is the Young/Daly period for $A_{i}$.
  \end{itemize}
  \item To decide which application is given priority among all $r+s$ candidates applications in $\Catiocat \cup \Catckptcat$, we select the one that minimizes the expected total waste induced by this choice, as explained below.
  \end{itemize}

 \subsubsection{Selection among candidate applications}

At the current time-step, there are $r+s$ candidates in $\Catiocat \cup \Catckptcat$:
\begin{itemize}
  \item Application $A_{i} \in \Catiocat$, $1\leq i \leq r$,
  has an I/O request of volume $v_{i}$ and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ initiated its I/O request $d_{i}$ seconds ago, and has been idle since $d_{i}$ seconds.
 \item Application $A_{i} \in  \Catckptcat$ has a checkpoint of duration $C_{i}$ seconds,
  and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ took its last checkpoint
  $d_{i}$ seconds ago, and keeps executing until it can checkpoint. For the record, we must have $d_{i} \geq \period{Daly}(A_{i})$
  since $A_{i}$ is a candidate.
  \end{itemize}

If we select application $A_{i}$ to perform I/O,  the expected waste $\wap{i}$ incurred
to the other $r+s-1$ candidate applications in  $\Catiocat \cup \Catckptcat$ is computed as follows.
Assume first that $A_{i} \in \Catiocat$. Then  $A_{i}$ will use the I/O resource for $v_{i}$ seconds.
\begin{itemize}
  \item Every other application $A_{j} \in \Catiocat$ will stay idle for $v_{i}$ additional seconds,
  hence its waste $\wapp{i}{j}$ is
  $$\wapp{i}{j} = q_{j} (d_{j} + v_{i})$$
  since there are $q_{j}$ processors enrolled in $A_{j}$ and idle for $d_{j} + v_{i}$ seconds. Note that for $A_{j} \in \Catiocat$, the waste $\wapp{i}{j}$ is deterministic.
  \item Every application $A_{j} \in \Catckptcat$ will continue executing for $v_{i}$ additional seconds, hence will be exposed to the risk of a failure that will strike within $v_{i}/2$ seconds on average. The probability of such a failure is $v_{i}/\mu_{j}$, where $\mu_{j}$ is the
  MTBF of application $A_{j}$. Since $A_{j}$ enrolls $q_{j}$ processors, we have $\mu_{j} = \muind/q_{j}$, where $\muind$ is the individual MTBF per processor. With this probability,
  the $q_{j}$ processors will have to recover and re-execute $d_{j} + v_{i}/2$ seconds of work,
  hence the waste $\wapp{i}{j}$ is
     $$\wapp{i}{j} = \frac{v_{i}}{\mu_{j} } q_{j} (R_{j} + d_{j} + \frac{v_{i}}{2}) =
     \frac{v_{i}}{\muind} q^{2}_{j} (R_{j} + d_{j} + \frac{v_{i}}{2})$$
     where $R_{j}$ is the recovery time for $A_{j}$.
Note that for $A_{j} \in \Catckptcat$, the waste $\wapp{i}{j}$ is probabilistic.
 \end{itemize}
 Altogether, the expected waste $\wap{i}$ incurred
to the other $r+s-1$ candidate applications is
$$\wap{i} = \sum_{A_{j} \in \Catiocat, j\neq i} \wapp{i}{j} + \sum_{A_{j} \in \Catckptcat} \wapp{i}{j}$$
We obtain
\begin{equation}
\label{eq.selection}
 \wap{i} = v_{i} \times \left( \sum_{1 \leq j \leq r, j\neq i} q_{j} (d_{j} + v_{i})
 + \sum_{r+1 \leq j \leq r+s}   \frac{q^{2}_{j}}{\muind} (R_{j} + d_{j} + \frac{v_{i}}{2}) \right)
\end{equation}

 Assume now that the selected application $A_{i} \in \Catckptcat$. Then  $A_{i}$ will use the I/O resource for $C_{i}$ seconds instead of $v_{i}$ seconds for $A_{i} \in \Catiocat$. We directly obtain the counterpart of Equation~\eqref{eq.selection} for its waste $\wap{i}$:
 \begin{equation}
\label{eq.selection2}
 \wap{i} = C_{i} \times \left( \sum_{1 \leq j \leq r} q_{j} (d_{j} + C_{i})
 + \sum_{r+1 \leq j \leq r+s, j\neq i}   \frac{q^{2}_{j}}{\muind} (R_{j} + d_{j} + \frac{C_{i}}{2}) \right)
\end{equation}

 Finally, we select the application $A_{i} \in \Catiocat \cup \Catckptcat$ whose waste
 $\wap{i}$ is minimal.


%\subsubsection{Selection in category \IOcat}
%\label{sec.iocat}
%
%  Let $(A_{i})_{1 \leq i \leq m}$ be the application candidates of category \IOcat.
%  Application $A_{i}$ has an I/O request of volume $v_{i}$ and enrolls $q_{i}$ processors.
%  Choosing $A_{i}$ makes every other candidate application $A_{j}$, $j \neq i$, keep $q_{j}$ processors idle during a time
%  proportional to $v_{i}$, so we choose $i$ that minimizes
%  $$(\sum_{j \neq i} q_{j}) \times v_{i}$$
%
%\subsubsection{Selection in category \Ckptcat}
%\label{sec.ckptcat}
%
%Let $(A_{i})_{1 \leq i \leq m}$ be the application candidates of category \Ckptcat.
%  Application $A_{i}$ has a checkpoint of duration $C_{i}$ seconds,
%  and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ took its last checkpoint
%  $d_{i}$ seconds ago (and for the record, we must have $d_{i} \geq \period{Daly}(A_{i})$
%  since $A_{i}$ is a candidate).
%   Choosing $A_{i}$ puts every other candidate application $A_{j}$, $j \neq i$,
%   at the risk of a failure that will strike within $C_{i}/2$ seconds on average.
%   Let $\bar{Q}_{i} = \sum_{j \neq i}q_{j}$ be the total number of
%   processors belonging to applications that want to checkpoint.
%   With probability $C_{i}/(\muind/\bar{Q}_{i})$ there will be a fault on one of these
%   processors. Here $\muind$ is the individual MTBF, hence we divide it by $\bar{Q}_{i}$
%   to get the MTBF over all processors at risk.
%
%   With probability $q_{j}/\bar{Q}_{i}$ the fault will strike application $A_{j}$ and incur a waste of duration $d_{j} + C_{i}/2$ for each of its $q_{j}$ processors. Altogether, the
%   expected amount of wasted time is
%   $$\frac{C_{i}}{(\muind/\bar{Q}_{i}) } \times \sum_{j \neq i}\frac{q_{j}}{\bar{Q}_{i}}(d_{j}+ \frac{C_{i}}{2})q_{j} = \frac{C_{i}}{\muind} \sum_{j \neq i} q_{j}^{2}(d_{j}+ \frac{C_{i}}{2})$$
%    and we choose $i$ that minimizes the above quantity.
