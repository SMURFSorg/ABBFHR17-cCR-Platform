% !TEX root =  ipdps18.tex

\section{I/O Scheduling Algorithms}
\label{sec:algorithms}
% Aurelien & George

In this section, we present the applications I/O scheduling algorithms used to study
the effects of concurrent I/O activities.  The first algorithm (\nocoop) represents
the status-quo in which application I/O activities are scheduled independently and
may incur slowdowns due to I/O resource contentions. \dca{I think we be careful of
  the term ``non-cooperative'', which to some can suggest adversarial and
  maliciious. Oblvious or independent is much more accurate, the I/O schedulers are
  simply indifferent, not malicious. Or we should at least define precisely what we
  mean when we use the term ``non-cooperative''.}

The second
algorithm (\fifoblock) coordinates applications to eliminate interference
between their I/O activities: only one application performs I/O at any given
time while other applications requesting I/O are blocked until their
turn comes (First Come First Serve, FCFS). The third algorithm (\fifononblock) is similar, except
that applications that are waiting for the I/O token
continue computing until their turn comes; note that unlike the
blocking algorithms, this optimization requires
application code refactoring. Last, we propose an heuristic
(\leastwaste) that improves on \fifononblock by giving the I/O token
to the application that imposes the least overhead on the system. Before
presenting the algorithms, we further discuss the interactions between
the checkpointing policy, the I/O workloads, and the interferences that the
algorithms have to consider.

%Instead of following a FCFS order to select the next I/O application,
%for each requesting application, the heuristic computes the prospective
%waste incurred by delaying its I/O (considering checkpoint and
%probabilistic recovery costs, idle time, etc.) when selecting another
%application, and selects the one that minimizes the waste increase
%at the current instant.


\subsection{Non-cooperative \nocoop I/O Scheduling}

In the non-cooperative I/O scheduling \nocoop, applications
fill-up the system based on processor count availability, and their I/O
workload (including checkpointing activities) are not organized by any
comprehensive system. Instead, applications use the PFS assuming they
are the sole user, and do not modify their access pattern to accommodate
for possible interferences. In these conditions, it has been observed~\cite{Dorier2015}
that concurrent access
to I/O resources will cause a decrease in the application observed
I/O bandwidth.
% One can imagine multiple cost functions for the effect of that sharing;
When the filesystem is under-provisioned, the overall throughput of the platform
would be maintained when multiple applications concurrently access I/O, and each
application should thereby observe a linear decrease in its own bandwidth. As an
application blocks on the I/O completion before it can continue, the decrease in
observed bandwidth leads to a proportionate increase in I/O time that must be
accounted as waste.

Checkpoints are scheduled accordingly to the normal rearming strategy
(\ie after each checkpoint completion, the next checkpoint is scheduled
to start after $\period{i}-\ckpt{i}$). Given that checkpoints may actually
last longer than $\ckpt{i}$, the resultant period may be longer than
$\period{i}$. This is however consistent with the concept of a
checkpointing strategy that is applied blindly, without consideration
for issues stemming from I/O resource sharing and interferences.

In the \nocoop algorithm, we consider two variants where checkpoints are
tentatively taken at 1) fixed frequency (\propfixed), or 2) at the
Daly frequency (\propdaly).

\subsection{Blocking \fifoblock FCFS I/O Scheduling}
\label{sec:fcfsblock}

A simple optimization to the aforementioned scheme is to favor one of
the applications' I/O request over all others. While the overall throughput
may remain unchanged (given an efficient PFS implementation), the favored
application completes its I/O workload faster (\ie, at nominal speed
$\ckpt{i}$ for an application of class $\app{i}$).
Applications are selected to perform I/O in the order of their requests,
\ie ,as soon as an application starts blocking on an I/O operation it will
take a place in the back of an  I/O queue. When the currently
active application completes its I/O, the next application waiting for I/O
is granted access, and starts making progress. We denote this scheme as
\fifoblock.

The advantage of \fifoblock over \nocoop can be seen in a simple workload with two
applications, assuming a favorable linear interference model.
If the two applications simultaneously start an I/O requesting the
transfer of a similar volume $V$ of data, in the \nocoop strategy,
both applications take $\frac{V}{\frac{\bandavail}{2}}$ time to complete
their I/O. In the \fifoblock strategy, the first (as serialized when
inserting in the queue) application takes only $\frac{V}{\bandavail}$
as it enjoys exclusive access to the PFS, while the second application
waits $\frac{V}{\bandavail}$ before its own I/O starts, but then in turn
also complete at nominal speed $\frac{V}{\bandavail}$, and therefore still
completes in $\frac{2V}{\bandavail}$. Thanks
to reducing I/O interferences, the average I/O completion time
has been reduced for the applications (although fairness has been decreased).

Similarly to the previous strategy, application observed checkpoint
duration may increase past $\ckpt{i}$, no longer because of interference, but now because
of  the time spent waiting for the first place in the I/O FCFS scheduling.
As we have computed above, although the average $C-\ckpt{i}$
difference is lower, for one application it is as large as in the
previous strategy. In any case, again, the checkpointing period will
be, in average, larger than the desired $\period{i}$ period.

In the \fifoblock algorithm, we again consider two variants, where checkpoints are
tentatively taken at 1) fixed frequency (\bfifofixed), or 2) at the
Daly frequency (\bfifodaly).

\subsection{Non-Blocking \fifononblock FCFS I/O Scheduling}
\label{sec:fcfsnonblock}

In the previous strategy, the cost of I/O interferences has been
exchanged for idle time when waiting for the I/O token in a blocking
fashion. If the application developer can refactor the code
to continue computing while awaiting for the I/O request to be granted,
it becomes possible to overlap the idle time with useful computation.
Indeed, checkpointing I/O operations can
be effectively time-shifted, at the risk of increasing the exposure to failures.
 In the \fifononblock algorithm, when the previous checkpoint ends at time $t_{now}$, 
 a tentative date for the next checkpoint is set at $t_{req}=t_{now}+\period{i}-\ckpt{i}$.
 When the application reaches date $t_{req}$, a non-blocking I/O request
is made and reserves a position in the
I/O queue. The application keeps computing until the
scheduler informs the application that the I/O system is exclusively
available to the application. Then, the application initiates its
I/O (checkpointing, initial input, final output or recovery). When the active application completes
its I/O, the next requesting application (in FCFS order)
become the active I/O application.

When an application is informed that it can use the I/O system to
checkpoint, the checkpointing library or application mechanism
in charge of synchronizing the checkpoints may immediately (or after
a short synchronization) start producing the checkpoint I/O workload
(typical for system-based checkpoint),
or finish the current computing block before allowing the checkpoint
to proceed (typical for user-level checkpoint). In this work, we consider
that this resynchronization cost is negligible with respect to the
checkpoint duration. Should a failure impact that application,
it would restart from the date at which the last checkpoint was taken, and
not at $t_{req}$, which is another improvement when compared to the
\fifoblock and \nocoop algorithms.
%\todo[inline]{If we talk about the positive aspect we might also want to mention the negative one, when a fault trigger during the I/O introduced delay}.

Again, we consider two variants in the \fifononblock algorithm where checkpoints are
tentatively taken at 1) fixed frequency (\fifofixed), or 2) at the
Daly frequency (\fifodaly).

%Aurelien: talked with Thomas and this is not what we want to study here.
% However, that
% state can be initially captured by copy-on-write mechanisms, or stored
% in local memory or in compute node-local burst buffers (\eg local SSD
% drives). Although node-local burst buffers do not offer protection
% against faults, they permit offsetting the transfer of the checkpoint
% data to a later date when the I/O token is available to the application.
% When the application finaly gets the token, the previously scratch-space
% stored checkpoint is transfered to the PFS without interference.

%NOTODO: something about replacing with last ckpt if token doesn't come in fast enough;
% there's something that doesn't work with the T-C after C depiction: we would rollback unbounded amounts now.
% that's because we do not consider whats commented down here with local scratchpads
% the checkpoint is taken at a date t_c posterior to t_req, and we will restart at t_c, not t_req.

\subsection{\leastwaste Algorithm}
\label{sec:least-waste}

The \leastwaste algorithm further refines on the \fifononblock algorithm
by giving the I/O token to the application that generates the least
waste, rather than simply in requesting order. Note that given the time-dependent nature of that decision, the selection may
not be the global optimum, but only an approximation given currently
available information about the system status.
In the \leastwaste algorithm, whenever an I/O operation completes at time $t$,
we consider a pool of application candidates from two different categories:
\begin{compactitem}
 \item Category \IOcat $\Catiocat$: Applications $A_{i}$, $1\leq i \leq r$, which
 need to do input I/O, output I/O or recovery
 \item Category \Ckptcat $\Catckptcat$: Applications $A_{i}$, $r+1\leq i \leq r+s$,
 whose last checkpoint took place no later than time $t - \period{Daly}(A_{i})$, where
 $\period{Daly}(A_{i})$ is the Young/Daly period for $A_{i}$.
\end{compactitem}

To decide which application is given priority among all $r+s$ candidates
applications in $\Catiocat \cup \Catckptcat$, we select the one that
minimizes the total expected waste induced, as explained hereafter.
At the current time-step, there are $r+s$ candidates in $\Catiocat \cup \Catckptcat$:
\begin{compactitem}
%
  \item Application $A_{i} \in \Catiocat$, $1\leq i \leq r$, has an I/O request
  of volume $v_{i}$ and enrolls $q_{i}$ processors. At the current time-step,
  $A_{i}$ initiated its I/O request $d_{i}$ seconds ago, and has been idle since
  $d_{i}$ seconds.
%
 \item Application $A_{i} \in  \Catckptcat$ has a checkpoint of duration $C_{i}$
 seconds, and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ took
 its last checkpoint $d_{i}$ seconds ago, and keeps executing until it can
 checkpoint. For the record, we must have $d_{i} \geq \period{Daly}(A_{i})$
 since $A_{i}$ is a candidate.
%
\end{compactitem}

If we select application $A_{i}$ to perform I/O, the expected waste $\wap{i}$
incurred to the other $r+s-1$ candidate applications in  $\Catiocat \cup
\Catckptcat$ is computed as follows. Assume first that $A_{i} \in \Catiocat$.
Then  $A_{i}$ will use the I/O resource for $v_{i}$ seconds.
\begin{compactitem}
%
  \item Every other application $A_{j} \in \Catiocat$ will stay idle for $v_{i}$
  additional seconds, hence its waste $\wapp{i}{j}$ is $$\wapp{i}{j} = q_{j}
  (d_{j} + v_{i})$$ since there are $q_{j}$ processors enrolled in $A_{j}$ that
  idle for $d_{j} + v_{i}$ seconds. Note that for $A_{j} \in \Catiocat$, the
  waste $\wapp{i}{j}$ is deterministic.
%
  \item Every application $A_{j} \in \Catckptcat$ will continue executing for
  $v_{i}$ additional seconds, hence will be exposed to the risk of a failure
  that will strike within $v_{i}/2$ seconds on average. The probability of such
  a failure is $v_{i}/\mu_{j}$, where $\mu_{j} =
  \muind/q_{j}$. With this
  probability, the $q_{j}$ processors will have to recover and re-execute $d_{j} +
  v_{i}/2$ seconds of work, hence the waste $\wapp{i}{j}$ is $$\wapp{i}{j} =
  \frac{v_{i}}{\mu_{j} } q_{j} (\reco{j} + d_{j} + \frac{v_{i}}{2}) =
  \frac{v_{i}}{\muind} q^{2}_{j} (\reco{j} + d_{j} + \frac{v_{i}}{2})$$ where
  $\reco{j}$ is the recovery time for $A_{j}$. Note that for $A_{j} \in
  \Catckptcat$, the waste $\wapp{i}{j}$ is probabilistic.
%
 \end{compactitem}
 Altogether, the expected waste $\wap{i}$ incurred
to the other $r+s-1$ candidate applications is
$$\wap{i} = \sum_{A_{j} \in \Catiocat, j\neq i} \wapp{i}{j} + \sum_{A_{j} \in \Catckptcat} \wapp{i}{j}$$
We obtain
\begin{equation}
\label{eq.selection}
\begin{array}{ll}
 \wap{i} = & v_{i} \times \left( \sum_{1 \leq j \leq r, j\neq i} q_{j} (d_{j} + v_{i}) \right.\\
& + \left. \sum_{r+1 \leq j \leq r+s}   \frac{q^{2}_{j}}{\muind} (\reco{j} + d_{j} + \frac{v_{i}}{2}) \right)
 \end{array}
\end{equation}

 Assume now that the selected application $A_{i} \in \Catckptcat$. Then  $A_{i}$ will use the I/O resource for $\ckpt{i}$ seconds instead of $v_{i}$ seconds for $A_{i} \in \Catiocat$. We directly obtain the counterpart of Equation~\eqref{eq.selection} for its waste $\wap{i}$:
 \begin{equation}
\label{eq.selection2}
 \begin{array}{ll}
 \wap{i} = & \ckpt{i} \times \left( \sum_{1 \leq j \leq r} q_{j} (d_{j} + \ckpt{i}) \right.\\
& + \left. \sum_{r+1 \leq j \leq r+s, j\neq i}   \frac{q^{2}_{j}}{\muind} (\reco{j} + d_{j} + \frac{C_{i}}{2}) \right)
 \end{array}
\end{equation}

 Finally, we select the application $A_{i} \in \Catiocat \cup \Catckptcat$ whose waste
 $\wap{i}$ is minimal.
Contrarily to the previous scenarios, we do not consider the variant where the checkpointing frequency is arbitrarily set, because the \leastwaste algorithm is designed to optimize checkpoint frequencies across applications.
