\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}
\DeclareMathOperator{\lcm}{lcm}
\usepackage{paralist}
\usepackage{color}
\usepackage{xspace}
\usepackage{amsthm}

\usepackage{algorithm}
\usepackage{algorithmicx}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\newcommand{\muind}{\mu_{\text{ind}}}
\newcommand{\bandtotal}{\beta_{\text{tot}}}
\newcommand{\bandavail}{\beta_{\text{avail}}}
\newcommand{\appset}{{\mathcal A}}
\newcommand{\nbnodesplat}{{\mathcal N}}
\newcommand{\nbapps}{|{\mathcal A}|}
\newcommand{\app}[1]{A_{#1}}
\newcommand{\application}[2]{a_{#1}^{#2}}
\newcommand{\nbapp}[1]{n_{#1}}
\newcommand{\nbnodes}[1]{q_{#1}}
\newcommand{\period}[1]{P_{#1}}
\newcommand{\ckpt}[1]{C_{#1}}
\newcommand{\wasteapp}[1]{W_{#1}}
\newcommand{\mtbfplat}{\mu}
\newcommand{\wasteplat}{W}
\newcommand{\ioconstraint}{\mathit{IO}}
\newcommand{\lastckpt}[2]{L_{#1}^{#2}}
\newcommand{\wastefct}[2]{W_{#1}(#2)}
\newcommand{\pool}{{\mathcal P}}
\newcommand{\risk}{{\textsc Risk}}
\newcommand{\todo}[1]{\textit{TBD: [#1]}}

\newcommand{\IOcat}{\textsc{IO-Candid}\xspace}
\newcommand{\Ckptcat}{\textsc{Ckpt-Candid}\xspace}

\author{Dorian Arnold, George Bosilca, Aurelien Bouteiller,\\
 Kurt Feirrera, Thomas Herault, Yves Robert}

\title{Optimal Cooperative Checkpointing for Shared HPC Platforms}

\begin{document}

\maketitle

\section{Status}

After several discussions at UTK recently, here is the current status for the simulator
and for the paper.

\subsection{Scheduling applications}

\begin{itemize}
  \item We take a large number of applications. These applications belong to a few classes
  (say 4 classes). We enforce the percentage per class that is reported in the APEX paper.
  Each application comes with: class, number of processors, input I/O size, output I/O size, 
  checkpoint size,  CPU time (equal to wall-time reported in APEX paper). To avoid side effects
  induced by having hundreds
  of completely identical applications,
  we use normal distributions for CPU time with mean equal to original APEX value and small standard deviation (say 10\%). 
  \item We shuffle the applications and present them all in a big queue to the scheduler, 
  which uses a simple first-fit.
  algorithm. No user reservation, plain greedy.
  \item When an application fails, we resubmit it will new wall-time equal to the fraction that remains since the last checkpoint.  Input I/O becomes recovery. Output I/O is not modified. 
  Now we reschedule every application that has not started yet, and we put the
  failed application at the head of the queue so that the scheduler will consider it first now.
\end{itemize}

\subsection{Dealing with interferences for non-cooperative application scheduling}
\begin{itemize}
  \item For coherence, we use the same policy for input I/O, output I/O and checkpoint operations 
  \item Blocking: only one application can perform I/O operations at the given time-step.
  I/O requests are served on a FCFS basis. When an application has requested an I/O operation, it waits idle until being served.
  \item Sharing: the I/O resource is shared at very time-step to serve all current requests. Sharing is fair, i.e., bandwidth is inversely proportional to number of requests. 
  \item Here is an example:
  application $A_{1}$ wants to perform five gigabytes of I/O at time $t$, 
  and available bandwidth is 
  $1$ gigabyte per second. If there is no other request, the operation will take five seconds.
  Now application $A_{2}$ (with same number of processors as $A_{1}$, hence with same assigned bandwidth in the sharing policy) wants to perform two gigabytes of I/O at time $t+1$.
  \begin{itemize}
  \item Blocking: $A_{2}$'s operation is delayed until time $t+5$ and will complete at $t+7$.
  \item Sharing: Instead of taking $2$ seconds, $A_{2}$'s operation will take $4$ seconds, because the bandwidth is shared with $A_{1}$ during its whole transfer. As a results, $A_{2}$'s operation completes at $t+5$ while $A_{1}$'s  operation is slowed down during four units and now completes at $t+7$.
  \end{itemize}
\end{itemize}

\subsection{Orchestrating I/O for cooperative application scheduling}

\begin{itemize}
  \item We always use the blocking strategy but not the FCFS policy.
  \item Instead, whenever an I/O operation completes at time $t$, we have a pool of application candidates:
  \begin{itemize}
   \item Category \IOcat: Applications $A_{i}$ which need to do input I/O, output I/O or recovery
  \item Category \Ckptcat: Applications $A_{i}$ whose last checkpoint took place not later than time $t - \period{Daly}(A_{i})$, where $\period{Daly}(A_{i})$ is the Young/Daly period for $A_{i}$.
  \end{itemize}
  \item Category \IOcat applications are given priority. If several of them are present, we choose the one
  which minimizes the total idle time incurred by the other applications, see Section~\ref{sec.iocat}.
  \item Category \Ckptcat applications are selected whenever there is no candidate of category \IOcat .
  If several of them are present, we choose the one
  which minimizes the total expected  waste of resources incurred by the other applications,
  see Section~\ref{sec.ckptcat}.
\end{itemize}


\subsubsection{Selection in category \IOcat}
\label{sec.iocat}
  
  Let $(A_{i})_{1 \leq i \leq m}$ be the application candidates of category \IOcat. 
  Application $A_{i}$ has an I/O request of volume $v_{i}$ and enrolls $q_{i}$ processors.
  Choosing $A_{i}$ makes every other candidate application $A_{j}$, $j \neq i$, keep $q_{j}$ processors idle during a time
  proportional to $v_{i}$, so we choose $i$ that minimizes 
  $$(\sum_{j \neq i} q_{j}) \times v_{i}$$
    
\subsubsection{Selection in category \Ckptcat}
\label{sec.ckptcat}

Let $(A_{i})_{1 \leq i \leq m}$ be the application candidates of category \Ckptcat. 
  Application $A_{i}$ has a checkpoint of duration $C_{i}$ seconds,
  and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ took its last checkpoint 
  $d_{i}$ seconds ago (and for the record, we must have $d_{i} \geq \period{Daly}(A_{i})$
  since $A_{i}$ is a candidate).
   Choosing $A_{i}$ puts every other candidate application $A_{j}$, $j \neq i$, 
   at the risk of a failure that will strike within $C_{i}/2$ seconds on average.
   Let $\bar{Q}_{i} = \sum_{j \neq i}q_{j}$ be the total number of
   processors belonging to applications that want to checkpoint.
   With probability $C_{i}/(\muind/\bar{Q}_{i})$ there will be a fault on one of these
   processors. Here $\muind$ is the individual MTBF, hence we divide it by $\bar{Q}_{i}$
   to get the MTBF over all processors at risk. 
   
   With probability $q_{j}/\bar{Q}_{i}$ the fault will strike application $A_{j}$ and incur a waste of duration $d_{j} + C_{i}/2$ for each of its $q_{j}$ processors. Altogether, the
   expected amount of wasted time is 
   $$\frac{C_{i}}{(\muind/\bar{Q}_{i}) } \times \sum_{j \neq i}\frac{q_{j}}{\bar{Q}_{i}}(d_{j}+ \frac{C_{i}}{2})q_{j} = \frac{C_{i}}{\muind} \sum_{j \neq i} q_{j}^{2}(d_{j}+ \frac{C_{i}}{2})$$
    and we choose $i$ that minimizes the above quantity.
    
    
   \section{Introduction}

This paper investigates scheduling strategies to checkpoint several applications
that execute concurrently on a shared HPC (High-Performance Computing) platform.
Sharing HPC platforms is the norm: running a capability application that spans the entire platform
is the exception, and execution logs show that several smaller-size applications typically share the platform~\cite{xx}.

With several applications executing concurrently, the de-facto standard execution mode on today's platforms is to let each application checkpoint independently. There are two widely-used approaches: (i) to use the same checkpoint period, typically one hour, for each application; (ii) to use a checkpoint period specific to each application, based upon its checkpoint time and its number of processors. In the second approach (ii), the well-known Young/Daly formula~\cite{young74,daly04} states that is is optimal to checkpoint an application
every $\sqrt{2 \mu C}$ seconds, where $C$ is the checkpoint time and $\mu$ the application MTBF.
Here the application MTBF $\mu$ is equal to $\mu = \frac{\muind}{q}$ where $q$ is the number of processors enrolled by the application and $\muind$ the MTBF of an individual processor~\cite{springer-monograph}. As a consequence, both $\mu$ and $C$ 
in the Young/Daly formula are application-dependent, and optimal periods can be quite different
over the application spectrum.

While both approaches (i) and (ii) are routinely used on shared HPC platforms, they incur a
big resource waste, because they lead to a very inefficient usage of an already scarce resource, namely the I/O system. There are two major reasons for that:
\begin{itemize}
\item First, the I/O system itself may not be capable to sustain the I/O requirements from all the applications. Let $\bandtotal$ be the total filesystem I/O bandwidth. Unless the platform was ill-provisioned, the value of $\bandtotal$ should allow for the possibility to checkpoint all platform nodes
in reasonable time. However, the applications sharing the platform perform regular (non-checkpoint)
I/O operations throughout execution, so that only a fraction $\bandavail$ of the total  bandwidth remains available for checkpoints, and this fraction may not be sufficient, in particular when some applications perform
intensive non-checkpoint I/O and others have large checkpoint files.
\item Second, and more importantly, the applications execute independently, so there is a high probability that their checkpoints will overlap. Consider the simple case where two applications checkpoint simultaneously a file of same size. Each will be assigned half the fraction $\bandavail$
to checkpoint, so that both checkpoints will be twice longer than expected. Such interferences dramatically decrease the throughput of the platform.
\end{itemize}

The objective of this paper is to assess the impact of such interferences, and to design scheduling algorithms that squeeze the most out of the I/O bandwidth available for checkpoints. We introduce cooperative algorithms where applications checkpoint in sequence, with a frequency that
is dictated by the priority they dynamically receive from the scheduler. When there is enough I/O bandwidth available, each application checkpoints with its optimal Young/daly period. When it is not the case, the scheduling algorithm provides the optimal checkpoint instants for each application, in order to maximize the platform throughput.

The main contributions of the paper are: ...
The rest of the paper is organized as follows ...

%Consider a large-scale platform with several applications executing
%concurrently. All these applications routinely perform I/O operations
%throughout their execution. The average fraction of I/O bandwidth that
%remains available can be used for checkpointing. Ideally, each application $A_{i}$
%should checkpoint, during a time $C_{i}$ , 
%every $P_{i}$ units of time. Here $P_{i}$ is the length of the
%optimal checkpointing period given by the Young/Daly formula~\cite{young74,daly04}:
%$$P_{i} = \sqrt{2 \mu_{i} C_{i}}$$
%where $\mu_{i}$ is the application MTBF, which is inversely
%proportional to the number of processors enrolled in its execution.
% 
%However, with each application having a different $P_{i}$, lasting and
%starting for and at arbitrary times (a behavior we call
%uncooperative), nothing prevents the checkpoint of an application to
%occur while another competitively does I/O (because of its normal
%application behavior, or because of a checkpoint). Because this
%introduces interferences between I/O (\cite{interference}), the time
%to complete both the checkpoint of the first application and the
%competing I/O of the second are adversely impacted. When many
%applications execute an I/O operation competitively, all of them can
%be impacted, reducing the efficiency of each.
%
%Moreover, checkpointing each application with optimal period $P_{i}$
%is possible only if enough I/O bandwidth is available. If this is the
%case, the impact of failures is kept to a minimum using Daly's period
%for each application.  However, if I/O bandwidth is limited, either in
%the absolute (imbalanced hardware design) or in the current
%co-execution (because a few applications need to consume a large
%fraction of bandwidth to progress), applications have to checkpoint
%less frequently.  All of them? if not, which ones? what are the
%optimal checkpointing periods in this context of co-scheduling with a
%given bound on available I/O bandwidth, and how to schedule the
%checkpoints in order to minimize interferences and optimize resource
%spent doing I/O? This paper answers these important questions.

\section{Related work}
\label{sec.related}

We survey related work in this section. We first discuss papers related to I/O pressure 
dur to checkpointing. Then we deal with I/O interference.

\subsection{Checkpointing and I/O}

For a single application, the optimal checkpointting period is given by the Young/Daly
formula~\cite{young74,daly04}. This period minimizes the platform waste, defined as 
the fraction of the
execution time that does not contribute to the progress of the application (the
time \emph{wasted}).  There are two sources of waste, the time spent taking checkpoints
(which calls for longer checkpoint periods),
and the time needed to recover and re-execute after each failure
(which calls for shorter checkpoint periods),
The Young/Daly
period acheives the optimal trade-off between both sources to minimize the 
total waste.
However, this optimal period may put too much pressure 
on the I/O system. It is possible to use a longer, sub-optimal, period that would
less pressure and still lead to a reasonable waste? S. Arunagiri et al.~\cite{Arunagiri2009} have studied such trade-offs and they have shown, both analytically and instantiating the model with four real-life platforms,
that a great decrease in I/O requirement can be achieved  at the price of a small increase of the waste.



\subsection{I/O interference}


\section{Execution Model}
\label{sec.model}

\todo{Question: how can we consider applications with finite time, initial
  input and final output? Current idea is to distribute complete
  volume of I/O over wall time, then take a single fake schedule that
  assign resources to the apps following a distribution, and say 'it
  shouldn't be far from finite apps being scheduled eagerly over
  finite resource for a long time, in average'.}

In this work, we consider a shared platform. The platform consists of
a set of computing nodes, an interconnection network, and storage
resources in the form of a parallel filesystem. Applications are
scheduled on the platform by a job scheduler. They access their
computing resource exclusively, but share the network and parallel
filesystem. They do regular Input/Output operations during their
execution, load an input file at startup and dump an output file at
completion. Because applications last long, and the platform is
subject to failures, they use periodic coordinated checkpoint with
rollback recovery to ensure their resilience.

Applications are of different sizes: they use a different number of
computing nodes, their I/O pattern and requirement vary, their
duration vary, and the size of their checkpoints vary. To model this
behavior with a few parameters, we do a set of simplifying assumptions
that we validate in the experimental section:
\begin{itemize}
  \item There is a large number of applications, but a small number of
    application classes (set of applications that behave similarly in
    terms of number of nodes, I/O requirements, duration and
    checkpoint size);
  \item The non-checkpoint-related I/O of applications is distributed
    over their entire execution duration equally;
  \item Applications may have different execution times, but we consider a finite
    segment of a representative schedule where the number of applications in each class remains constant. This is the key to analyze and optimize the steady-state I/O behavior of the 
    application portfolio. 
\end{itemize}

Formally, we consider a set $\appset$ of $\nbapps$ applications
classes $\app{1}, \ldots \app{\nbapps}$ that run in parallel in a
parallel machine with $\nbnodesplat$ nodes. Application class
$\app{i}$ represents $\nbapp{i}$ applications that each use
$\nbnodes{i}$ nodes
($\sum_{i}\nbapp{i} \nbnodes{i} = \nbnodesplat$), and checkpoints
periodically with period $\period{i}$, in a time $\ckpt{i}$ when there
is no interference with other I/O operation.

\section{Optimal Checkpointing Period under I/O constraint}
\label{sec.optimal}

The waste of an application is the ratio of time that the application spends doing
resilience operations by the time that it does useful work. The time
spent doing resilience operations include he time spend during each period to
checkpoint, and in case of failure, the time of computation that was
lost and the time to rollback from the previous checkpoint. We assume
that the rollback time is equivalent to the checkpoint time, and we
can define the waste $\wasteapp{i}$ of an application of class
$\app{i}$ as follows (\cite{springer-monograph}):

\begin{equation}
\wasteapp{i} = \wastefct{i}{\ckpt{i}} = \frac{\ckpt{i}}{\period{i}} +
\frac{\nbnodes{i}}{\mtbfplat}(\frac{\period{i}}{2} + \ckpt{i})
\label{eq.wasteAi}
\end{equation}

Let $\wasteplat$ be the waste of the platform. We define this as the
weighted arithemetic mean of the $\wasteapp{i}$ for all applications
(where each application is weighted by the number of computing nodes
it uses):

\begin{equation}
\wasteplat = \sum_i \frac{\nbapp{i} \nbnodes{i}}{\nbnodesplat} \wasteapp{i}
\label{eq.waste}
\end{equation}

In the absence of I/O constraints, the checkpointing period can be minimized
for each application independently. Indeed, the optimal period for an application of 
of class $\app{i}$ is obtained by minimizing $\wasteapp{i}$ in Equation~\eqref{eq.wasteAi}.
Differentiating and solving 
$$\frac{\delta \wasteapp{i}}{\delta \period{i}} = - \frac{\ckpt{i}}{\period{i}^{2}} + \frac{\nbnodes{i}}{2 \mtbfplat} = 0$$
we readily derive that
\begin{equation}
\period{i} = \sqrt{2 \frac{\mtbfplat}{\nbnodes{i}} \ckpt{i}} = \sqrt{2 \mu_{i} \ckpt{i}}
\label{eq.daly}
\end{equation}
where $\mu_{i}$ is the MTBF of  class $\app{i}$ applications, which is the Young/Daly formula~\cite{young74,daly04}.

However, I/O constraints impose to use sub-optimal periods. If each application
of  class $\app{i}$ checkpoints in time $\ckpt{i}$ during its period $\period{i}$ (hence without any contention), it uses the I/O device during a fraction $\frac{\ckpt{i}}{\period{i}}$ of the time.
The total usage fraction of the  I/O device is $\sum_{i} \frac{\nbapp{i} \ckpt{i}}{\period{i}}$
and cannot exceed $1$. Therefore, we have to solve the following optimization problem: find
the set of values $\period{i}$ that minimize $\wasteplat$ in Equation~\eqref{eq.waste} subject to the I/O constraint:

\begin{equation}
\sum_{i} \frac{\nbapp{i} \ckpt{i}}{\period{i}} \leq 1
\label{eq.IOconstraint}
\end{equation}

Note that Equation~\eqref{eq.IOconstraint} is a necessary condition, but is may not be sufficient:
even though the total I/O bandwidth is not exceeded, meaning there is enough capacity to take all the checkpoints at the given periods, we still need to orchestrate these checkpoints so that there is no contention. We come back to this point below.

The optimization problem writes: minimize $\wasteplat = \sum_i \frac{\nbapp{i} \nbnodes{i}}{\nbnodesplat}  \left( \frac{\ckpt{i}}{\period{i}} +
\frac{\nbnodes{i}}{\mtbfplat}(\frac{\period{i}}{2} + \ckpt{i}) \right)$
subject to $\ioconstraint = \sum_{i} \frac{\nbapp{i} \ckpt{i}}{\period{i}} \leq 1$.
Using the Karush-Kuhn-Tucker condition, we know that there exists a nonnegative constant 
$\lambda$
such that 
$$- \frac{\delta \wasteplat}{\delta \period{i}} = \lambda \frac{\delta\ioconstraint }{\delta \period{i}}$$
for all $i$. We derive that 
$$\frac{\nbapp{i} \nbnodes{i} \ckpt{i}}{\nbnodesplat \period{i}^{2}} -    \frac{\nbapp{i} \nbnodes{i}^{2}}{2 \mtbfplat \nbnodesplat} = - \lambda \frac{\nbapp{i} \ckpt{i}}{\period{i}^{2}}
$$
for all $i$. This leads to:
 \begin{equation}
\period{i} = \sqrt{\frac{2 \mtbfplat  \nbnodesplat}{\nbnodes{i}^{2}} \left(\frac{\nbnodes{i}}{\nbnodesplat} +\lambda \right) \ckpt{i}}
  \label{eq.KKT}
\end{equation}
for all $i$. Note that when $\lambda=0$, Equation~\eqref{eq.KKT} reduces to Equation~\eqref{eq.daly}. Because of the I/O constraint in Equation~\eqref{eq.IOconstraint},
we will choose for $\lambda$ the minimum value such that Equation~\eqref{eq.IOconstraint}
  is satisfied. This will lead to periods $P_{i}$ larger than the optimal value of Equation~\eqref{eq.daly}. 
  Note that there is no closed-form expression for the minimum value of $\lambda$,
  it has to be found numerically.
   Altogether, we state our main result:
   
   \begin{theorem}
  In the presence of I/O constraints, the optimal values of the checkpointing periods are given
  by Equation~\eqref{eq.KKT}, where $\lambda$ is the smallest nonnegative value such that
  Equation~eq\ref{eq.IOconstraint} holds.
\end{theorem}

\section{Optimal Cooperative Checkpointing Strategy}
\label{sec.strategy}

\subsection{With Burst Buffers}

With burst buffers, $\ckpt{i}$ still represents the time it takes to
upload the checkpoint to the stable storage (the file system).
Consider Algorithm~\ref{alg.withbb}: every $\period{i}$ time
units, each application of class $\app{i}$ takes a checkpoint and save
it onto the next free slot on the burst buffer; a global shared FIFO
queue transfers checkpoints from active slots in burst buffers onto
the parallel file system following the FIFO queue order.

\algblockdefx{Process}{EndProcess}[1]{\textbf{On Process} #1}{\textbf{End Process}}
\algnotext{EndProcess}
\algblockdefx{Every}{DoneEvery}[1]{\textbf{Every } #1}{\textbf{Done}}
\algnotext{DoneEvery}
\algblockdefx{When}{DoneWhen}[1]{\textbf{When } #1}{\textbf{Done}}
\algnotext{DoneWhen}
\algloopdefx{If}[1]{\textbf{If} #1 \textbf{then}}
\begin{algorithm}
\caption{Cooperative Checkpointing Algorithm with Burst Buffers}
\label{alg.withbb}
\begin{algorithmic}
\State \textbf{var} $transfer\_queue$, a FIFO initially empty
\Process{$p$, belonging to application $\application{i}{j}$ of class $\app{i}$}
   \State \textbf{var} $slots$ set of burst buffer files that can
   store a checkpoint 
   \Every{$\period{i}$ time units}
           \State $slot  \gets $ oldest slot that is not being used for transfer
           \State Checkpoint application state into $slot$
           \If{$slot$ is marked done transferring}
                  \State Append $(p, slot)$ to $transfer\_queue$
    \DoneEvery
\EndProcess

\Process{$T$}
   \When{$transfer\_queue$ is not empty}
       \State Pop $slot$ from $transfer\_queue$
       \State Mark $slot$ as being transferred
       \State Transfer Checkpoint in $slot$ to File System
       \State Mark $slot$ as done transferring
   \DoneWhen
\EndProcess
\end{algorithmic}
\end{algorithm}

\begin{theorem}
  Algorithm~\ref{alg.withbb} ensures that all applications of class $\app{i}$
  checkpoint at most every $max(\sum_j\nbapp{j}*\ckpt{j}, \period{i})$
  time units, and requires only a burst buffer capable of storing 2
  checkpoints per process.
\end{theorem}

\begin{proof}
  \todo{This derives from $\sum_i \frac{\ckpt{i}}{\period{i}} \leq 1$,
    but should be done properly.}
\end{proof}
    
The theorem does provide a lower bound for the platform waste.
However, we have a FIFO system, and some applications may incur
a re-execution time larger than $P_{i}$. What is the worst case?
A first optimization is the following: when a checkpoint is taken
    by a given application, we check whether its previous checkpoint is
    still in the queue from the burst buffer to the file system. If yes, the new
checkpoint should \emph{replace} the old one, keeping the same position in the queue.
With this optimization, there is at most two checkpoints per application in the queue,
one being currently transferred and one waiting.
Then the worst case is to wait for the checkpoint of all the other applications.
Formally, the maximal re-execution time for application
$\application{i}{j}$ of class
$\app{i}$ is
$$\max(P_{i}, \sum_{k=1}^{\nbapps} n_{k}C_{k} - C_{i})$$

 \todo{Discussed at JLESC meeting: size of burst buffer can be
    bounded by 2 checkpoints easily, and it should be: if there are 3
    checkpoints in the burst buffer, then the first one might be being
    transfered, but this means that the 2nd is useless as we already
    reached the 3rd one. The 2nd should be discarded and its slot in
    the FIFO queue should be taken by the 3rd.}

\subsection{Without Burst Buffers}

Without a caching mechanism, the times at which to take the checkpoint
must be scheduled to avoid any kind of checkpoint-checkpoint
interference (which can only waste resources, as all interfering
applications are slowed down while blocking on non-productive
operations). We consider here a centralized scheduler that decides at
any time what next application should checkpoint, and when.

The scheduler remembers when each application $\application{i}{j}$ of class
$\app{i}$ last initiated a checkpoint. We then define $\lastckpt{i}{j}$
as the time since the last checkpoint $\application{i}{j}$ started
(or since the start of $\application{i}{j}$ if none has been taken yet). 
As soon as $\application{i}{j}$ has executed for $P_{i}$ time-units,
it is put in the checkpointing pool $\pool$ by the centralized scheduler. It 
continues executing until it is selected from the pool to checkpoint.

Which application in the pool should be selected to checkpoint?
Assume that some previous checkpoint terminates at time $t$,
and let 
$$\pool = \{ \application{i_{1}}{j_{1}}, \dots, \application{i_{k}}{j_{k}} \}$$
be the set of applications  in the pool at time $t$. All these applications are 
candidate to checkpointing. At time $t$, each application $\application{i_{\ell}}{j_{\ell}} \in \pool$
has been executing for  $\lastckpt{i_{\ell}}{j_{\ell}}$ time-units.

If we select $\application{i_{\ell}}{j_{\ell}}$ to checkpoint (in time $C_{j_{\ell}}$),
and if there is a failure during that checkpoint, the time lost by every other application
$\application{i_{m}}{j_{m}} \in \pool$, $m \neq \ell$, is (in expectation) equal to
 $\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2}$. 
We define the risk incurred by application
$\application{i_{m}}{j_{m}} \in \pool$, $m \neq \ell$
as its potential waste, which is the time lost divided by its MTBF $\frac{\mtbfplat}{\nbnodes{i}}$,
times the number $\nbnodes{i_m}$ processors enrolled by this application, i.e.
$$ \frac{\nbnodes{i_{m}}}{\mtbfplat}  (\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2})
\times \nbnodes{i_m} $$
Altogether, selecting $\application{i_{\ell}}{j_{\ell}}$ to checkpoint leads to a total risk
$$\risk(\application{i_{\ell}}{j_{\ell}}) = \sum_{1 \leq m \leq k, m \neq \ell} \frac{\nbnodes{i_{m}}^{2}}{\mtbfplat}  \times (\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2})$$
  for all applications that stayed in the pool.
  
We greedily choose the application in the pool that minimizes the risk:
$$\application{i_{\ell}}{j_{\ell}} = Argmin_{\application{i_m}{j_m} \in \pool} \risk(\application{i_m}{j_m})$$

%At the end of each checkpointing, the scheduler selects
%$\application{i}{j}$ such that 
%$$
%\left\{
%\begin{array}{l}
%\nbnodes{i}(\frac{\wastefct{i}{\lastckpt{i}{j}}}{\wastefct{i}{\period{i}}}) \textrm{ if }\lastckpt{i}{j}\geq\period{i}\\
%0\textrm{ if }\lastckpt{i}{j}<\period{i}\\
%\end{array}\right.$$
%is maximal and strictly superior to 0.

Two remarks:
\begin{itemize}
\item Because we put applications in the pool only after they have run for $P_{i}$ times-steps,
this greedy algorithm guarantees Young/Daly periods to every application
whenever there is non conflict to access I/O resources.
\item The final schedule is not periodic. Instead, it is constructed dynamically
after each checkpoint completion. Computing the actual waste can be achieved 
through simulation, and compared to the lower bound of Section~\ref{sec.optimal}. 
\end{itemize}


\bibliographystyle{abbrv}
\bibliography{biblio}
\end{document}