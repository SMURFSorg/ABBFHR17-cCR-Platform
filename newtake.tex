\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}
\DeclareMathOperator{\lcm}{lcm}
\usepackage{paralist}
\usepackage{color}
\usepackage{xspace}
\usepackage{amsthm}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\newcommand{\appset}{{\mathcal A}}
\newcommand{\nbnodesplat}{{\mathcal N}}
\newcommand{\nbapps}{|{\mathcal A}|}
\newcommand{\app}[1]{A_{#1}}
\newcommand{\application}[2]{a_{#1}^{#2}}
\newcommand{\nbapp}[1]{n_{#1}}
\newcommand{\nbnodes}[1]{q_{#1}}
\newcommand{\period}[1]{P_{#1}}
\newcommand{\ckpt}[1]{C_{#1}}
\newcommand{\wasteapp}[1]{W_{#1}}
\newcommand{\mtbfplat}{\mu}
\newcommand{\wasteplat}{W}
\newcommand{\lastckpt}[2]{L_{#1}^{#2}}
\newcommand{\wastefct}[2]{W_{#1}(#2)}
\newcommand{\pool}{{\mathcal P}}
\newcommand{\risk}{{\textsc Risk}}
\newcommand{\todo}[1]{\textit{TBD: [#1]}}

\author{Dorian Arnold, George Bosilca, Aurelien Bouteiller,\\
 Kurt Feirrera, Thomas Herault, Yves Robert}

\title{Optimal Uncooperative Checkpointing for Shared HPC Platforms}

\begin{document}

\maketitle

\section{Introduction}

Consider a large-scale platform with several applications executing
concurrently. All these applications routinely perform I/O operations
throughout their execution. The average fraction of I/O bandwidth that
remains available can be used for checkpointing. Ideally, each application $A_{i}$
should checkpoint, during a time $C_{i}$ , 
every $P_{i}$ units of time. Here $P_{i}$ is the length of the
optimal checkpointing period given by the Young/Daly formula~\cite{young74,daly04}:
$$P_{i} = \sqrt{2 \mu_{i} C_{i}}$$
where $\mu_{i}$ is the application MTBF, which is inversely
proportional to the number of processors enrolled in its execution.
 
However, with each application having a different $P_{i}$, lasting and
starting for and at arbitrary times (a behavior we call
uncooperative), nothing prevents the checkpoint of an application to
occur while another competitively does I/O (because of its normal
application behavior, or because of a checkpoint). Because this
introduces interferences between I/O (\cite{interference}), the time
to complete both the checkpoint of the first application and the
competing I/O of the second are adversely impacted. When many
applications execute an I/O operation competitively, all of them can
be impacted, reducing the efficiency of each.

Moreover, checkpointing each application with optimal period $P_{i}$
is possible only if enough I/O bandwidth is available. If this is the
case, the impact of failures is kept to a minimum using Daly's period
for each application.  However, if I/O bandwidth is limited, either in
the absolute (imbalanced hardware design) or in the current
co-execution (because a few applications need to consume a large
fraction of bandwidth to progress), applications have to checkpoint
less frequently.  All of them? if not, which ones? what are the
optimal checkpointing periods in this context of co-scheduling with a
given bound on available I/O bandwidth, and how to schedule the
checkpoints in order to minimize interferences and optimize resource
spent doing I/O? This paper answers these important questions.

\section{Execution Model}
\label{sec.model}

\todo{Question: how can we consider applications with finite time, initial
  input and final output? Current idea is to distribute complete
  volume of I/O over wall time, then take a single fake schedule that
  assign resources to the apps following a distribution, and say 'it
  shouldn't be far from finite apps being scheduled eagerly over
  finite resource for a long time, in average'.}

In this work, we consider a shared platform. The platform consists of
a set of computing nodes, an interconnection network, and storage
resources in the form of a parallel filesystem. Applications are
scheduled on the platform by a job scheduler. They access their
computing resource exclusively, but share the network and parallel
filesystem. They do regular Input/Output operations during their
execution, load an input file at startup and dump an output file at
completion. Because applications last long, and the platform is
subject to failures, they use periodic coordinated checkpoint with
rollback recovery to ensure their resilience.

Applications are of different sizes: they use a different number of
computing nodes, their I/O pattern and requirement vary, their
duration vary, and the size of their checkpoints vary. To model this
behavior with a few parameters, we do a set of simplifying assumptions
that we validate in the experimental section:
\begin{itemize}
  \item there is a large number of applications, but a small number of
    application classes (set of applications that behave similarly in
    terms of number of nodes, I/O requirements, duration and
    checkpoint size);
  \item the non-checkpoint-related I/O of applications is distributed
    over their entire execution duration equally;
  \item Applications are long-lasting, and we consider a finite
    segment of a single representative schedule.
    \todo{I can't write that properly... Need rework}
\end{itemize}

Formally, we consider a set $\appset$ of $\nbapps$ applications
classes $\app{1}, \ldots \app{\nbapps}$ that run in parallel in a
parallel machine with $\nbnodesplat$ nodes. Application class
$\app{i}$ represents $\nbapp{i}$ applications that each use
$\nbnodes{i}$ nodes
($\sum_{i}\nbapp{i} \nbnodes{i} = \nbnodesplat$), and checkpoints
periodically with period $\period{i}$, in a time $\ckpt{i}$ when there
is no interference with other I/O operation.

\section{Optimal Checkpointing Period under I/O constraint}
\label{sec.optimal}

The waste of an application is the ratio of time that the application spends doing
resilience operations by the time that it does useful work. The time
spent doing resilience operations include he time spend during each period to
checkpoint, and in case of failure, the time of computation that was
lost and the time to rollback from the previous checkpoint. We assume
that the rollback time is equivalent to the checkpoint time, and we
can define the waste $\wasteapp{i}$ of an application of class
$\app{i}$ as follows (\cite{springer-monograph}):

$$\wasteapp{i} = \wastefct{i}{\ckpt{i}} = \frac{\ckpt{i}}{\period{i}} +
\frac{\nbnodes{i}}{\mtbfplat}(\frac{\period{i}}{2} + \ckpt{i})$$

Let $\wasteplat$ be the waste of the platform. We define this as the
weighted arithemtic mean of the $\wasteapp{i}$ for all applications
(where each application is weighted by the number of computing nodes
it uses):

$$\wasteplat = \sum_i \frac{\nbapp{i} \nbnodes{i}}{\nbnodesplat} \wasteapp{i}$$

\todo{Explain why $\sum_i \frac{\ckpt{i}}{\period{i}}\leq 1$}

The Optimal Checkpointing Period under I/O constraint is the set of
$\period{i}$ that minimize $\wasteplat$, knowing that $\sum_i
\frac{\ckpt{i}}{\period{i}} \leq 1$.

\todo{Use the Karush-Kuhn-Tucker approach to find optimal
  $\period{i}$ under the constraint above}

\section{Optimal Cooperative Checkpointing Strategy}
\label{sec.strategy}

\subsection{With Burst Buffers}

With burst buffers, $\ckpt{i}$ still represents the time it takes to
upload the checkpoint to the stable storage (the filesystem).
Consider the following algorithm: every $\period{i}$ time units, each
application of class $\app{i}$ takes a checkpoint on the burst buffer,
and adds the transfer of this checkpoint to the filesystem to a global
shared FIFO queue of transfers to execute.

\begin{theorem}
This algorithm ensures that all checkpoints are transferred to the
filesystem without interference between checkpoints and without
filling up the burst buffers
\end{theorem}

\begin{proof}
  \todo{This derives from $\sum_i \frac{\ckpt{i}}{\period{i}} \leq 1$,
<<<<<<< .mine
    but should be done properly. Yves will write it down soon.}
    \end{proof}
    
The theorem does provide a lower bound for the platform waste.
However, we have a FIFO system, and some applications may incur
a re-execution time larger than $P_{i}$. What is the worst case?
A first optimization is the following: when a checkpoint is taken
    by a given application, we check whether its previous checkpoint is
    still in the queue from the burst buffer to the file system. If yes, the new
checkpoint should \emph{replace} the old one, keeping the same position in the queue.
With this optimization, there is at most two checkpoints per application in the queue,
one being currently transferred and one waiting.
Then the worst case is to wait for the checkpoint of all the other applications.
Formally, the maximal re-execution time for application
$\application{i}{j}$ of class
$\app{i}$ is
$$\max(P_{i}, \sum_{k=1}^{\nbapps} n_{k}C_{k} - C_{i})$$
%|
%  \todo{Discussed at JLESC meeting: size of burst buffer can be
%    bounded by $2\ckpt{i}$ easily, and it should be: if there are 3
%    checkpoints in the burst buffer, then the first one might be being
%    transfered, but this means that the 2nd is useless as we already
%    reached the 3rd one. The 2nd should be discarded and its slot in
%    the FIFO queue should be taken by the 3rd.}

\subsection{Without Burst Buffers}

Without a caching mechanism, the times at which to take the checkpoint
must be scheduled to avoid any kind of checkpoint-checkpoint
interference (which can only waste resources, as all interfering
applications are slowed down while blocking on non-productive
operations). We consider here a centralized scheduler that decides at
any time what next application should checkpoint, and when.

The scheduler remembers when each application $\application{i}{j}$ of class
$\app{i}$ last initiated a checkpoint. We then define $\lastckpt{i}{j}$
as the time since the last checkpoint $\application{i}{j}$ started
(or since the start of $\application{i}{j}$ if none has been taken yet). 
As soon as $\application{i}{j}$ has executed for $P_{i}$ time-units,
it is put in the checkpointing pool $\pool$ by the centralized scheduler. It 
continues executing until it is selected from the pool to checkpoint.

Which application in the pool should be selected to checkpoint?
Assume that some previous checkpoint terminates at time $t$,
and let 
$$\pool = \{ \application{i_{1}}{j_{1}}, \dots, \application{i_{k}}{j_{k}} \}$$
be the set of applications  in the pool at time $t$. All these applications are 
candidate to checkpointing. At time $t$, each application $\application{i_{\ell}}{j_{\ell}} \in \pool$
has been executing for  $\lastckpt{i_{\ell}}{j_{\ell}}$ time-units.

If we select $\application{i_{\ell}}{j_{\ell}}$ to checkpoint (in time $C_{j_{\ell}}$),
and if there is a failure during that checkpoint, the time lost by every other application
$\application{i_{m}}{j_{m}} \in \pool$, $m \neq \ell$, is (in expectation) equal to
 $\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2}$. 
We define the risk incurred by application
$\application{i_{m}}{j_{m}} \in \pool$, $m \neq \ell$
as its potential waste, which is the time lost divided by its MTBF $\frac{\mtbfplat}{\nbnodes{i}}$,
times the number $\nbnodes{i_m}$ processors enrolled by this application, i.e.
$$ \frac{\nbnodes{i_{m}}}{\mtbfplat}  (\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2})
\times \nbnodes{i_m} $$
Altogether, selecting $\application{i_{\ell}}{j_{\ell}}$ to checkpoint leads to a total risk
$$\risk(\application{i_{\ell}}{j_{\ell}}) = \sum_{1 \leq m \leq k, m \neq \ell} \frac{\nbnodes{i_{m}}^{2}}{\mtbfplat}  \times (\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2})$$
  for all applications that stayed in the pool.
  
We greedily choose the application in the pool that minimizes the risk:
$$\application{i_{\ell}}{j_{\ell}} = Argmin_{\application{i_m}{j_m} \in \pool} \risk(\application{i_m}{j_m})$$

%At the end of each checkpointing, the scheduler selects
%$\application{i}{j}$ such that 
%$$
%\left\{
%\begin{array}{l}
%\nbnodes{i}(\frac{\wastefct{i}{\lastckpt{i}{j}}}{\wastefct{i}{\period{i}}}) \textrm{ if }\lastckpt{i}{j}\geq\period{i}\\
%0\textrm{ if }\lastckpt{i}{j}<\period{i}\\
%\end{array}\right.$$
%is maximal and strictly superior to 0.

Two remarks:
\begin{itemize}
\item Because we put applications in the ppol only after they have run for $P_{i}$ times-steps,
this greedy algorithm guarantees Young/Daly periods to every application
whenever there is non conflict to access I/O resources.
\item The final schedule is not periodic. Instead, it is constructed dynamically
after each checkpoint completion. Computing the actual waste can be achieved 
through simulation, and compared to the lower bound of Section~\ref{sec.optimal}. 
\end{itemize}


%\todo{Discussed at the JLESC meeting: this function is not right. We
%  now have a function based on $O(\lastckpt{i}{j}\nbnodes{i}^2)$, that
%  works with the following assumption: we wait that $\lastckpt{i}{j}
%  \geq \period{i}$ to prevent any increased waste due to greedy
%  checkpointing (this also guarantees Daly periods, if we have enough
%  slack to do them); then, once we reached at least $\period{i}$, the
%  goal is to minimize the waste due to the risk. And that reduces to
%  minimizing the risk weighted by the amount of resource at risk, which is in
%  $\frac{\nbnodes{i}}{\nbnodesplat}\cdot\frac{\nbnodes{i}}{\mtbfplat}(\lastckpt{i}{j} + \ckpt{i})$.}

\bibliographystyle{abbrv}
\bibliography{biblio}
\end{document}