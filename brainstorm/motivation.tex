\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}
\DeclareMathOperator{\lcm}{lcm}
\usepackage{paralist}

\author{Dorian Arnold, George Bosilca, Aurelien Bouteiller,\\
 Kurt Feirrera, Thomas Herault, Yves Robert}

\title{Coordinated Checkpointing: Coordinating at the Platform Level}

\begin{document}

\maketitle

\section{What we need from Kurt (Feb. 28, 2017)}

We need execution \emph{scenarios}.
Each scenario consists of the following information:
\begin{compactitem}
\item A platform $(p,\mu, BW_{io})$:\\
- total number of processors $p$\\
- individual processor MTBF $\mu$
or platform MTBF $\mu/p$\\
- total I/O bandwidth $BW_{io}$
\item A set of $n$ applications that execute in steady-state mode. 
For each application $A_{i} = (q_{i}, CS_{i}, bw_{i})$, we need:\\
- the number of processors $q_{i}$\\
- the checkpoint size $CS_{i}$ (in gigabytes)\\
- the (averaged) I/O bandwidth fraction used to perform application-related I/O, 
not counting checkpointing\\
\end{compactitem}

Note: With all this data, we can compute the checkpoint time $C_{i}$
of each application $A_{i}$ as $C_{i} = CS_{i}/bw_{avail}$
where $bw_{avail} = BW_{io} - \sum_{i=1}^{n} bw_{i}$.\\

Note: If some parameters are missing, we will \emph{adopt} reasonable values.
This is easy for the MTBF but more difficult for the I/O consumptions $bw_{i}$.
For each application, we could divide total IO volume by execution duration if 
these two parameters are known.

\section{Motivation}

Consider a platform that is not well balanced to do capacity checkpointing: either because the applications are doing interfering I/O, or because the I/O is a bottleneck. Production runs can still benefit significantly from CR techniques. For example, take a platform with the following characteristics:

\begin{center}
\begin{tabular}{l|c|l}
  Number of nodes & $p$             & $10^5$ \\\hline
  Node MTBF           & $\mu_{ind}$ & 25 years\\\hline
  Node memory      & $M$             & 32GB\\\hline
  Checkpoint Ratio & $\rho$         & 33\%\\\hline
  Filesystem Bandwidth & $\beta$ & 10GB/s\\
\end{tabular}
\end{center}

The time to checkpoint a capacity run is $p\frac{M \rho}{\beta} = 29.63h$. The MTBF of the machine is $\frac{\mu_{ind}}{p} = 2.19h$. Thus, one cannot use the first order approximation of Daly to find the optimal checkpointing time and the waste of the machine at this capacity. Since failures hit more often than it is possible to checkpoint at this size, the waste would be very close to one, because in average the application would not progress between failures (to give an example, an execution of 40 min
followed by a checkpoint, hence 30 hours total, takes $10^{26}$ hours to complete successfully in expectation).

If the machine is used at capacity for a large amount of smaller jobs, however, each application could be protected using a CR technique. Let's consider a toy example:

\begin{center}
\begin{tabular}{lcl}
  Total number of processors & $p$ & $100,000$\\\hline
  Number of 'big' applications & $n_{b}$ & 10\\\hline
  Size of 'big' applications & $b$ & 5,000\\\hline
  Number of 'small' applications & $n_{s}$ & 500\\\hline
  Size of 'small' applications & $s$ & 100\\\hline
  Ratio of the machine used by 'big' applications & $\alpha = \frac{b n_{b}}{p} $ & 50\%\\
\end{tabular}
\end{center}

Then, if there was no filesystem contention/interference, and all applications could checkpoint in parallel, we could apply the Daly formulae:

\begin{eqnarray}
C(q) &=& \frac{q\rho M}{\beta}\\
\mu(q) &=& \frac{\mu_{ind}}{q}\\
Waste(q, T) &=& \frac{C(q)}{T} + \frac{1}{\mu(q)}\left(\frac{T}{2}+R(q)\right)\\
Daly(q) &=& \sqrt{2\mu(q)C(q)} = \sqrt{\frac{2\mu_{ind}\rho M}{\beta}}
\end{eqnarray}

We assume that $R(q) =C(q)$ for the recovery cost. Note that the period $Daly(q)=Daly$ is independent of
$q$, because checkpoint time is linear in $q$ while MTBF is inversely proportional to $q$.

This would give $Waste(b, Daly(b)) = 29.2\%$, and $Waste(s, Daly(s)) = 0.5\%$, which are reasonable values applications can expect. From a platform perspective, we can define the waste of the platform,

$$Waste_{plat}(T_s, T_b) = \frac{s n_{s}}{p}Waste(s, T_s) + \frac{b n_{b}}{p}Waste(b, T_b)$$

In the ideal case, $Waste_{ideal} = Waste_{plat}(Daly(s), Daly(b)) = 14.9\%$.

However, to achieve this performance, applications must checkpoint every $Daly \approx 11h23min$ (with a $\mu_{ind}$ of 25 years).
Since there are $n_b = \frac{\alpha p}{b} = 10$ 'big' applications and $n_s = \frac{(1-\alpha)p}{s} = 500$ 'small' applications, and 'big' applications take $C(b) = 1h28min$ while 'small' applications take $C(s) = 1min47s$ to checkpoint on the filesystem without interference, the time it takes to checkpoint all applications once without interference is $T_{min} = n_bC(b) + n_sC(s) = C(p) >> Daly(b) = Daly(s)$.

If we apply a simple round-robin strategy, allowing each application to checkpoint in turn, we obtain

$$
\begin{array}{rl}
Waste(s, T_{min}) &= 0.77\%\\
Waste(b, T_{min}) &= 41.8\%\\
\end{array}
$$

Although the small applications are not really impacted by a much larger period, big ones are impacted significantly. The platform waste using the round robin strategy in that case is $Waste_{rr} = Waste_{plat}(T_{min}, T_{min}) = 21.5\%$, $1.44$ times higher than the ideal waste.

Based on this observation, one can decide to checkpoint more often the big applications: if every $n_bC(b)$ checkpoints of big applications we checkpoint only a tenth of the small applications, the period for big applications becomes $U = n_bC(b)+50C(s) = 16.29h$, and the period for small applications becomes $10U$, and the platform waste $Waste_{unfair} = Waste_{plat}(U, 10U) = 17.25\%$ ($31\%$ for large applications, $3.7\%$ for small applications, global waste $1.174$ times higher than the ideal waste). Thus, hindering small applications in the case of I/O interference in favor of big ones turns out positive for the global platform efficiency in that case. Note that if 'small' applications last less than 6 days, in practice, not providing resilience techniques for these applications at all can be more beneficial from the platform point of view.

The goal of this paper is to find an efficient platform priority scheduling, when it is not possible to accommodate the optimal checkpointing period of all applications that run in parallel.


\section{General pattern (updated Feb. 28, 2017, by Thomas and Yves)}

Consider $n$ applications on a platform with $p$ processors of individual MTBF $\mu$.
 Application $A_{i}$ has $q_{i}$ processors and checkpoint time $C_{i}$. We can
 assume that $p = \sum_{i=1}^{n} q_{i}$.
Here we do not assume that $C_{i} = q_{i} C_{\textit{base}}$ because each application could save a different total volume of data. The MTBF of $A_{i}$ is $\mu_{i} = \frac{\mu}{q_{i}}$.

We aim at building a periodic pattern of length $T$ such that each $A_{i}$ checkpoints $d_{i}$ times
within the pattern. Thus $T  = \sum_{i=1}^{n} d_{i} C_{i}$, and the $d_{i}$ (which we assume
to take rational values instead of integer ones) are the unknown. 

Application $A_{i}$ has $d_{i}$ periods of length $T_{i } = w_{i}+C_{i}$, where 
$d_{i} T_{i} =T$
(we can assume equality, if we had idle time, we would increase the work chunk $w_{i}$, hence $T_{i}$).
What is the waste? The waste for $A_{i}$ is 
$$Waste_{i} = \frac{C_{i}}{T_{i}} + \frac{1}{\mu_{i}} (LostWork_{i} + R_{i})$$
and for the platform, $Waste = \sum_{i=1}^{n} \frac{q_{i}}{p} Waste_{i}$.
Assume $A_{i}$ can checkpoint without delay. Then $LostWork_{i} = \frac{T_{i}}{2}$.
This is  optimistic, because even though we can schedule the I/O, there is little chance that checkpoints can be equally spaced for each application.
We get
$$Waste_{i} = \frac{C_{i}}{T_{i}} + \frac{1}{\mu_{i}} (\frac{T_{i}}{2} + R_{i})
= \frac{C_{i} d_{i}}{T}+ \frac{q_{i}}{\mu} (\frac{T}{2d_{i}} + R_{i})$$
In that case we can solve and compute the $d_{i}$, at least numerically. 
Indeed, we we want to minimize 
$Waste = \frac{1}{p} \sum_{i=1}^{n} q_{i} Waste_{i}$, 
which is 
$$Waste = \frac{1}{p} \sum_{i=1}^{n} q_{i} (\frac{C_{i} d_{i}}{T}+ \frac{q_{i}}{\mu} (\frac{T}{2d_{i}} + R_{i}))$$
where $T  = \sum_{i=1}^{n} d_{i} C_{i}$.

We introduce the variables $m_{i} = \frac{C_{i} d_{i}}{T}$ to rewrite the waste as
$$Waste = \frac{1}{p} \sum_{i=1}^{n} q_{i} (m_{i}+ \frac{q_{i}C_{i}}{2 \mu m_{i}} +\frac{q_{i} R_{i}}{\mu})$$
subject to $\sum_{i=1}^{n} m_{i} = 1$.
Using the Lagrange multiplier $\gamma$, we minimize
$$Waste - \gamma (\sum_{i=1}^{n} m_{i} - 1)$$
by zeroing out the $n$ partial derivates for each $m_{i}$ and $\gamma$ and obtain
$q_{i} - \frac{q_{i}^{2} C_{i}}{2 \mu m_{i}^{2}} - \gamma = 0$ for all $i$ (and the constraint
$\sum_{i=1}^{n} m_{i} = 1$ for $\gamma$). This leads to 
$$m_{i} = \sqrt{\frac{q_{i}^{2} C_{i}}{2 \mu (q_{i} - \gamma)}}$$
and we find $\gamma$ numerically from $\sum_{i=1}^{n} m_{i} = 1$.

Once we get the $m_{i}$, we can compute $T$ to ensure that each application checkpoints an integer number of times $d_{i}$ during the period $T$. Let $m_{i} = \frac{u_{i}}{v_{i}}$
where $u_{i}$ and $v_{i}$ are relatively prime integers, we have 
$d_{i} = \frac{T m_{i}}{C_{i}} = T \frac{u_{i}}{v_{i} C_{i}}$
so that choosing $T = \lcm_{1 \leq i \leq n}(v_{i} C_{i})$ ensure that application $A_{i}$ checkpoints an integer number of times during period $T$. Of course this will lead to huge values of $T$
and we will approximate the optimal solution (by rounding the $m_{i}$'s to 
reduce the period length.

Going back to the original example with $n_{b}$ large jobs with $b$ processors
and $n_{s}$ small jobs with $s$ processors,
we have $n = n_{s} + n_{b}$ applications, $p =  s n_{s} + b n_{b}$ processors. We get
$C_{s} = s C$ and $C_{b} = b C$ where $C = \frac{q\rho M}{\beta}$ is the time to checkpoint one processor. We derive that 
$$m_{s} = \sqrt{\frac{s^{3} C}{2 \mu (s - \gamma)}}, \qquad m_{b} = \sqrt{\frac{b^{3} C}{2 \mu (b - \gamma)}}$$
and solve numerically for $\gamma$. We derive that
$$d_{s} = \frac{T}{C} \frac{m_{s}}{s}, \qquad d_{b} = \frac{T}{C} \frac{m_{b}}{b}$$

In the example we had arbitrarily chosen $d_{b}/d_{s}=5$.
But here we find that $d_{b}/d_{s}=5.4$. Let's round this to $6$.
We get $T=6n_{b}C_{b}+n_{s}C_{s}=103.7h$, small application have period $T$
and large one have period $T/6$.  Total waste is $17.02\%$.
Note that if we round to ratio $5$, total waste becomes $17.03\%$.
OK, this is only slightly better than previous solution, but the mathematics 
 always win in the end.

\paragraph{With NVRAM}

Assume each node is equipped with NVRAM. Checkpoints are sent to NVRAM before
saved for real onto stable storage. Let $NV_{i}$ be the time to checkpoint on NVRAM; $C_{i}$
still is the time to checkpoint on stable storage. We expect $NV_{i} \ll C_{i}$.
The equations are updated as follows:
\begin{itemize}
\item We still have $T  = \sum_{i=1}^{n} n_{i} C_{i}$
\item  We now have $T_{i}= w_{i}+NV_{i}$ instead of $T_{i}= w_{i}+C_{i}$
but still $n_{i} T_{i}=T$, applications work more and the failure-free overhead is reduced
\item However, the overhead due to failures is unchanged, 
$LostWork_{i} = \frac{T_{i}}{2}$, 
because we need to recover from the last checkpoint saved on stable storage (NVRAM is gone with the crash of a node)
\end{itemize}
The solution is similar to above.

\end{document}
\pagebreak

\appendix
\section{Initial Thoughts}

Consider a platform with $p$ computing nodes. On this platform, at a given time job scheduler assigned $n$ applications that span on $S_{i, 1 \leq i \leq n}$ processors ($\sum_{i=1}^{n}S_i = p$). If each application uses coordinated checkpoint/restart and the same Daly formula for their checkpointing interval, is it possible to schedule all of them so that two applications never interfere?

Let $M$ be the average checkpoint size for a single node; let $b_{io}$ be the I/O bandwidth of the machine. For the sake of simplicity, consider first that there is no local I/O contention (the local link is not the bottleneck): Let $C(q)$ be the checkpoint duration of an application with $q$ nodes.

$$C(q) = q\times \frac{M}{b_{io}}$$

Let $\mu(q)$ be the MTBI of an application using $q$ nodes. By Daly, the optimal checkpoint interval of the application assuming no contention is $T_{opt}(q) = \sqrt{2C(q)\mu(q)}$. Assuming independent failures, $\mu(q) = \frac{\mu_{ind}}{q}$ (were $\mu_{ind}$ is the MTBF of a single node). Thus,

$$T_{opt}(q) = \sqrt{2\times q\times \frac{M}{b_{io}} \times \frac{\mu_{ind}}{q}} = \sqrt{2\frac{M}{b_{io}}\mu_{ind}} = T_{opt}$$

All applications checkpoint with the same interval, independent of their size (this is because the checkpoint duration is directly proportional to the number of nodes, while the chance of completing within the next time unit is inverse proportional to the same number of nodes).  To avoid contention, we thus must ensure that $\sum_{i = 1}^{n} C(S_i) \leq T_{opt}$, thus that

$$\begin{array}{rl}
\sum_{i = 1}^{n} S_i\times \frac{M}{b_{io}} & \leq \sqrt{2\frac{M}{b_{io}}\mu_{ind}}\\
p\frac{M}{b_{io}} & \leq \sqrt{2\frac{M}{b_{io}}\mu_{ind}}\\
b_{io} & \geq \frac{M\times p^2}{2\mu_{ind}}
\end{array}$$

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=.6\linewidth]{biomin.png}
  \end{center}
  \caption{Minimal platform I/O bandwidth requirement to allow independent checkpoint intervals\label{fig:minio}}
\end{figure}
 
Instantiating with $M = 8GB$, $p$ varying between 1,000 and 100,000, and $\mu_{ind}$ between 20 years and 80 years, we get Figure~\ref{fig:minio}. So... There is no problem? Indeed, if the platform can checkpoint a capacity application at the Daly optimal period, there is no problem. Projected machines seem to be able to do so for reasonable $\mu_{ind}$. Let's illustrate this in Figures~\ref{fig:csa-fs}, \ref{fig:csa-nvram}, \ref{fig:exa}: for a few machines, with a varying $\mu_{ind}$, this figure shows the waste using Daly optimal period (when defined) assuming the checkpoints use 60\% of the memory. Machines are described below:

\begin{description}
\item[Cori] Found in Jeff Vetters slides at the scheduling workshop: Memory is 1 PB, with 1.5PB of NVRAM. 9,300 nodes, 744GB/s I/O bandwidth. As the NVRAM is large enough to store twice 60\% of the main memory, we consider two subsystems: CoriNVRAM, where the checkpoints are stored scalably on NVRAM (at 10GB/s/node), and CoriFS, where checkpoints are stored into the main filesystem (at 744GB/s/system)
\item[Summit] Found in Jeff Vetters slides at the scheduling workshop: Memory is 1.74PB, with 2.8PB of NVRAM. 3,500 nodes, 1TB/s I/O bandwidth. As for Cori, we consider SummitNVRAM and SummitFS
\item[Aurora] Found in Jeff Vetters slides at the scheduling workshop: Memory is 7 PB, with 'on package memory local memory and persistant memory.' This is unclear what it means, so for AuroraNVRAM, we take the option 2.3PB main, 4.6PB NVRAM, and for AuroraFS, we take 7PB main. 50,000 nodes, 1TB/s I/O bandwidth.
\item[exa] Found in Jeff Vetters slides at the scheduling workshop: Memory is 32PB to 64PB, 100,000 to 1,000,000 nodes, I/O bandwidth between 30TB/s and 60TB/s. We consider the 8 options: exa1 (32PB, $10^5$, 30TB/s), exa2 (64PB, $10^5$, 30TB/s), exa3 (32PB, $10^6$, 30TB/s), exa4 (64PB, $10^6$, 30TB/s), exa5 (32PB, $10^5$, 60TB/s), exa6 (64PB, $10^5$, 60TB/s), exa7 (32PB, $10^6$, 60TB/s), exa8 (64PB, $10^6$, 60TB/s)
\end{description}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=.6\linewidth]{csa-fs.png}
\end{center}
\caption{Minimal platform checkpoint time on the filesystem for Cori, Summit and Aurora, as a ratio of the machine MTBF, and function of the MTBF of a single node (in s)\label{fig:csa-fs}}
\end{figure}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=.6\linewidth]{csa-nvram.png}
\end{center}
\caption{Minimal platform checkpoint time on their local NVRAM for Cori, Summit and Aurora, as a ratio of the machine MTBF and function of the MTBF of a single node (in s)\label{fig:csa-nvram}}
\end{figure}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=.6\linewidth]{exa.png}
\end{center}
\caption{Minimal platform checkpoint time on the filesystem for exascale putative machines, as a ratio of the machine MTBF and function of the MTBF of a single node (in s)\label{fig:exa}}
\end{figure}

\clearpage
\section{Another model}

Consider $n$ applications on a platform with $p$ processors of individual MTBF $\mu$.
 Application $A_{i}$ has $q_{i}$ processors and checkpoint time $C_{i}$. We can
 assume that $p = \sum_{i=1}^{n} q_{i}$.
Here we do not assume that $C_{i} = q_{i} C_{\textit{base}}$ because each application could save a different total volume of data. The MTBF of $A_{i}$ is $\mu_{i} = \frac{\mu}{q_{i}}$.

We aim at building a periodic pattern of length $T$ such that each $A_{i}$ checkpoints $n_{i}$ times
within the pattern. Thus $T  = \sum_{i=1}^{n} n_{i} C_{i}$, and the $n_{i}$ (which we assume
to take rational values instead of integer ones) are the unknown. 

Application $A_{i}$ has $n_{i}$ periods of length $w_{i}+C_{i}$, where $n_{i} (w_{i}+C_{i})=T$
(we can assume equality, if we had idle time, we would increase the work chunk $w_{i}$.
What is the waste? The waste for $A_{i}$ is 
$$Waste_{i} = \frac{n_{i} C_{i}}{T} + \frac{1}{\mu_{i}} (LostWork_{i} + R_{i})$$
and for the platform, $Waste = \sum_{i=1}^{n} \frac{q_{i}}{p} Waste_{i}$.


Assume $A_{i}$ can checkpoint without delay. Then $LostWork_{i} = \frac{1}{2} (w_{i}+C_{i}) = \frac{T}{2n_{i}}$.
This is  optimistic, because even though we can schedule the I/O, there is little chance that checkpoints can be equally spaced for each application.
Still, in that case we can solve and compute the $n_{i}$, at least numerically. Let $m_{i} =  \frac{n_{i} C_{i}}{T}$, we want to minimize 
$$Waste = \frac{1}{p} \sum_{i=1}^{n} q_{i} (m_{i} +  \frac{q_{i} C_{i}}{2 \mu m_{i}})$$ 
subject to $\sum_{i=1}^{n} m_{i} = 1$.

Using Lagrange multipliers, we minimize $Waste - \gamma (\sum_{i=1}^{n} m_{i} - 1)$
by zeroing out the $n$ partial derivates for each $m_{i}$ and $\gamma$ and obtain
$q_{i} - \frac{q_{i}^{2} C_{i}}{2 \mu m_{i}^{2}} - \gamma = 0$ for all $i$ (and the constraint
$\sum_{i=1}^{n} m_{i} = 1$ for $\gamma$). This leads to 
$$m_{i} = \sqrt{\frac{q_{i}^{2} C_{i}}{2 \mu (q_{i} - \gamma)}}$$
and we find $\gamma$ numerically from $\sum_{i=1}^{n} m_{i} = 1$.

\paragraph{With NVRAM}

Assume each node is equipped with NVRAM. Checkpoints are sent to NVRAM before
saved for real onto stable storage. Let $NV_{i}$ be the time to checkpoint on NVRAM; $C_{i}$
still is the time to checkpoint on stable storage. We expect $NV_{i} \ll C_{i}$.
The equations are updated as follows:
\begin{itemize}
\item We still have $T  = \sum_{i=1}^{n} n_{i} C_{i}$
\item  We now have $n_{i} (w_{i}+NV_{i})=T$ instead of $n_{i} (w_{i}+C_{i})=T$, applications work more and the failure-free overhead is reduced
\item However, the overhead due to failures is unchanged, $LostWork_{i} = \frac{T}{2n_{i}}$, 
because we need to recover from the last checkpoint saved on stable storage (NVRAM is gone with the crash of a node)
\end{itemize}
The solution is similar to above.


\clearpage
\section{Meeting January 24, 2017}

We had a quick meeting. Aur√©lien, Thomas and Yves asked a few questions to Kurt 
about the \emph{APEX workflows} paper. Here are a few items that were discussed:
\begin{itemize}
\item Workflows execute several \emph{pipelines}, meaning that the same task graph
is executed several times on different data sets. Each pipeline executes the whole
cycle of: (i) read input data; (ii) analyze and checkpoint periodically; (iii) store results.
\item Burst-buffers are widely used. We hope that they can allow to smooth out the requirement for
I/O operations, which are typically bursts of heavy activity followed by periods of little I/O
activity. In other words, if that's true, we could take the total I/O volume over the whole execution and derive an instantaneous I/O bandwidth usage for the workflow. 
\item That would simplify a lot the analysis.
Excluding fault-tolerance (checkpoints), input I/O and analysis I/O lead to a bandwidth fraction consumed by each workflow. Say 80\% total. We have to share what remains
(R=20\%)  for checkpointing the workflows. 
\item Now each workflow number $i$ has a checkpoint volume $V_{i}$ (fixed) and a checkpoint period $T_{i}$ (to be determined). It consumes 
a fraction $b_{i} = \frac{V_{i}}{T_{i} b_{io}}$. 
We have to ensure that $\sum_{i} b_{i} \leq R$.
\item Other questions:
\begin{itemize}
\item  Some sites allow, or encourage, users to reserve I/O nodes before execution.
Is that a general policy?
\item What are the co-scheduling strategies, if any, at the different sites? 
Which workflows execute concurrently?
\end{itemize}

\end{itemize}


\end{document}