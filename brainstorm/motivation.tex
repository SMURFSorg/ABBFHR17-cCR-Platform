\documentclass{article}

\usepackage{graphicx}

\author{Dorian Arnold, Aurelien Bouteiller, George Bosilca,\\
 Kurt Feirrera, Thomas Herault, Yves Robert}
\title{Coordinated Checkpointing: Coordinating at the Platform Level}

\begin{document}

\maketitle

Consider a platform that is not well balanced to do capacity checkpointing: either because the applications are doing interfering I/O, or because the I/O is a bottleneck. Production runs can still benefit significantly from cCR techniques. For example, take a platform with the following characteristics:

\begin{center}
\begin{tabular}{l|c|l}
  Number of nodes & $p$             & $10^5$ \\\hline
  Node MTBF           & $\mu_{ind}$ & 25 years\\\hline
  Node memory      & $M$             & 32GB\\\hline
  Checkpoint Ratio & $\rho$         & 33\%\\\hline
  Filesystem Bandwidth & $\beta$ & 10GB/s\\
\end{tabular}
\end{center}

The time to checkpoint a capacity run is $p\frac{M \rho}{\beta} = 29h20min$. The MTBF of the machine is $\frac{\mu_{ind}}{p} = 2h11min24s$. Thus, one cannot use the first order approximation of Daly to find the optimal checkpointing time and the waste of the machine at this capacity. Since failures hit more often than it is possible to checkpoint at this size, the waste would be one, as in average the application would not progress between failures.

If the machine is used at capacity for a large amount of smaller jobs, however, each application could be protected using a cCR technique. Let's consider a toy example:

\begin{center}
\begin{tabular}{lcl}
  Size of 'big' applications & $b$ & 5,000\\\hline
  Size of 'small' applications & $s$ & 100\\\hline
  Ratio of the machine used by 'big' applications & $\alpha$ & 50\%\\
\end{tabular}
\end{center}

Then, if there was no filesystem contention/interference, and all applications could checkpoint in parallel, we could apply the Daly formulae:

\begin{eqnarray}
C(q) &=& \frac{q\rho M}{\beta}\\
\mu(q) &=& \frac{\mu_{ind}}{q}\\
Waste(q, T) &=& \frac{C(q)}{T} + \frac{1}{\mu(q)}\left(\frac{T}{2}+C(q)\right)\\
Daly(q) &=& \sqrt{2\mu(q)C(q)}
\end{eqnarray}

Which would give $Waste(b, Daly(b)) = 26.1\%$, and $Waste(s, Daly(s)) = 0.5\%$, which are reasonable values applications can expect. From a platform perspective, we can define the waste of the platform, 

$$Waste_{plat}(T_s, T_b) = \frac{s}{p}Waste(s, T_s) + \frac{b}{p}Waste(b, T_b)$$

In the ideal case, $Waste_{ideal} = Waste_{plat}(Daly(s), Daly(b)) = 13.3\%$.

However, to achieve this performance, 'big' applications must checkpoint every $Daly(b) = 12h48min5.6s$, and 'small' applications must checkpoint every $Daly(s) = 11h21m51.2s$. Since there are $n_b = \frac{\alpha p}{b} = 10$ 'big' applications and $n_s = \frac{(1-\alpha)p}{s} = 500$ 'small' applications, and 'big' applications take $C(b) = 1h28min$ while 'small' applications take $C(s) = 16min16s$ to checkpoint on the filesystem without interference, the time it takes to checkpoint all applications once without interference is $T_{min} = n_bC(b) + n_sC(s) >> max(Daly(b), Daly(s))$.

If we apply a simple round-robin strategy, allowing each application to checkpoint in turn, we obtain

$$
\begin{array}{rl}
Waste(s, T_{min}) &= 0.76\%\\
Waste(b, T_{min}) &= 38.5\%\\
\end{array}
$$

Although the small applications are not really impacted by a much larger period, big ones are impacted significantly. The platform waste using the round robin strategy in that case is $Waste_{rr} = Waste_{plat}(T_{min}, T_{min}) = 19.63\%$, 1.47 times higher than the ideal waste.

Based on this observation, one can decide to checkpoint more often the big applications: if every $n_bC(b)$ checkpoints of big applications we checkpoint only a fifth of the small applications, the period for big applications becomes $U = n_bC(b)+100C(s) = 63360$, and the period for small applications becomes $5(n_bC(b)+100C(s)) = 316800$, and the platform waste $Waste_{unfair} = Waste_{plat}(U, 5U) = 15\%$ (1.12 times higher than the ideal waste). Thus, hindering small applications in the case of I/O interference in favor of big ones turns out positive for the global platform efficiency in that case.

The goal of this paper is to find an efficient platform priority scheduling, when it is not possible to accomodate the optimal checkpointing period of all applications that run in parallel.

\pagebreak

\appendix
\section{Initial Reflexion}

Consider a platform with $p$ computing nodes. On this platform, at a given time job scheduler assigned $n$ applications that span on $S_{i, 1 \leq i \leq n}$ processors ($\sum_{i=1}^{n}S_i = p$). If each application uses coordinated checkpoint/restart and the same Daly formula for their checkpointing interval, is it possible to schedule all of them so that two applications never interfere?

Let $M$ be the average checkpoint size for a single node; let $b_{io}$ be the I/O bandwidth of the machine. For the sake of simplicity, consider first that there is no local I/O contention (the local link is not the bottleneck): Let $C(q)$ be the checkpoint duration of an application with $q$ nodes.

$$C(q) = q\times \frac{M}{b_{io}}$$

Let $\mu(q)$ be the MTBI of an application using $q$ nodes. By Daly, the optimal checkpoint interval of the application assuming no contention is $T_{opt}(q) = \sqrt{2C(q)\mu(q)}$. Assuming independent failures, $\mu(q) = \frac{\mu_{ind}}{q}$ (were $\mu_{ind}$ is the MTBF of a single node). Thus,

$$T_{opt}(q) = \sqrt{2\times q\times \frac{M}{b_{io}} \times \frac{\mu_{ind}}{q}} = \sqrt{2\frac{M}{b_{io}}\mu_{ind}} = T_{opt}$$

All applications checkpoint with the same interval, independent of their size (this is because the checkpoint duration is directly proportional to the number of nodes, while the chance of completing within the next time unit is inverse proportional to the same number of nodes).  To avoid contention, we thus must ensure that $\sum_{i = 1}^{n} C(S_i) \leq T_{opt}$, thus that

$$\begin{array}{rl}
\sum_{i = 1}^{n} S_i\times \frac{M}{b_{io}} & \leq \sqrt{2\frac{M}{b_{io}}\mu_{ind}}\\
p\frac{M}{b_{io}} & \leq \sqrt{2\frac{M}{b_{io}}\mu_{ind}}\\
b_{io} & \geq \frac{M\times p^2}{2\mu_{ind}}
\end{array}$$

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=.6\linewidth]{biomin.png}
  \end{center}
  \caption{Minimal platform I/O bandwidth requirement to allow independent checkpoint intervals\label{fig:minio}}
\end{figure}
 
Instantiating with $M = 8GB$, $p$ varying between 1,000 and 100,000, and $\mu_{ind}$ between 20 years and 80 years, we get Figure~\ref{fig:minio}. So... There is no problem? Indeed, if the platform can checkpoint a capacity application at the Daly optimal period, there is no problem. Projected machines seem to be able to do so for reasonable $\mu_{ind}$. Let's illustrate this in Figures~\ref{fig:csa-fs}, \ref{fig:csa-nvram}, \ref{fig:exa}: for a few machines, with a varying $\mu_{ind}$, this figure shows the waste using Daly optimal period (when defined) assuming the checkpoints use 60\% of the memory. Machines are described below:

\begin{description}
\item[Cori] Found in Jeff Vetters slides at the scheduling workshop: Memory is 1 PB, with 1.5PB of NVRAM. 9,300 nodes, 744GB/s I/O bandwidth. As the NVRAM is large enough to store twice 60\% of the main memory, we consider two subsystems: CoriNVRAM, where the checkpoints are stored scalably on NVRAM (at 10GB/s/node), and CoriFS, where checkpoints are stored into the main filesystem (at 744GB/s/system)
\item[Summit] Found in Jeff Vetters slides at the scheduling workshop: Memory is 1.74PB, with 2.8PB of NVRAM. 3,500 nodes, 1TB/s I/O bandwidth. As for Cori, we consider SummitNVRAM and SummitFS
\item[Aurora] Found in Jeff Vetters slides at the scheduling workshop: Memory is 7 PB, with 'on package memory local memory and persistant memory.' This is unclear what it means, so for AuroraNVRAM, we take the option 2.3PB main, 4.6PB NVRAM, and for AuroraFS, we take 7PB main. 50,000 nodes, 1TB/s I/O bandwidth.
\item[exa] Found in Jeff Vetters slides at the scheduling workshop: Memory is 32PB to 64PB, 100,000 to 1,000,000 nodes, I/O bandwidth between 30TB/s and 60TB/s. We consider the 8 options: exa1 (32PB, $10^5$, 30TB/s), exa2 (64PB, $10^5$, 30TB/s), exa3 (32PB, $10^6$, 30TB/s), exa4 (64PB, $10^6$, 30TB/s), exa5 (32PB, $10^5$, 60TB/s), exa6 (64PB, $10^5$, 60TB/s), exa7 (32PB, $10^6$, 60TB/s), exa8 (64PB, $10^6$, 60TB/s)
\end{description}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=.6\linewidth]{csa-fs.png}
\end{center}
\caption{Minimal platform checkpoint time on the filesystem for Cori, Summit and Aurora, as a ratio of the machine MTBF, and function of the MTBF of a single node (in s)\label{fig:csa-fs}}
\end{figure}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=.6\linewidth]{csa-nvram.png}
\end{center}
\caption{Minimal platform checkpoint time on their local NVRAM for Cori, Summit and Aurora, as a ratio of the machine MTBF and function of the MTBF of a single node (in s)\label{fig:csa-nvram}}
\end{figure}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=.6\linewidth]{exa.png}
\end{center}
\caption{Minimal platform checkpoint time on the filesystem for exascale putative machines, as a ratio of the machine MTBF and function of the MTBF of a single node (in s)\label{fig:exa}}
\end{figure}


\end{document}