\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\DeclareMathOperator{\lcm}{lcm}
\usepackage{paralist}
\usepackage{color}
\usepackage[hidelinks]{hyperref}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{todonotes}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\newcommand*{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}} }
\newcommand{\ie}[0]{\emph{i.e.}\xspace}
\newcommand{\eg}[0]{\emph{e.g.}\xspace}

\newcommand{\muind}{\mu_{\text{ind}}}
\newcommand{\bandtotal}{\beta_{\text{tot}}}
\newcommand{\bandavail}{\beta_{\text{avail}}}
\newcommand{\appset}{{\mathcal A}}
\newcommand{\nbnodesplat}{{\mathcal N}}
\newcommand{\nbapps}{|{\mathcal A}|}
\newcommand{\app}[1]{A_{#1}}
\newcommand{\application}[2]{a_{#1}^{#2}}
\newcommand{\nbapp}[1]{n_{#1}}
\newcommand{\nbnodes}[1]{q_{#1}}
\newcommand{\period}[1]{P_{#1}}
\newcommand{\ckpt}[1]{C_{#1}}
\newcommand{\wasteapp}[1]{W_{#1}}
\newcommand{\wap}[1]{W_{#1}}
\newcommand{\wapp}[2]{W_{#1}(#2)}
\newcommand{\mtbfplat}{\mu}
\newcommand{\wasteplat}{W}
\newcommand{\ioconstraint}{\mathit{IO}}
\newcommand{\lastckpt}[2]{L_{#1}^{#2}}
\newcommand{\wastefct}[2]{W_{#1}(#2)}
\newcommand{\pool}{{\mathcal P}}
\newcommand{\risk}{{\textsc Risk}}
%\newcommand{\todo}[1]{\textit{TBD: [#1]}}

\newcommand{\IOcat}{\textsc{IO-Candidate}\xspace}
\newcommand{\Ckptcat}{\textsc{Ckpt-Candidate}\xspace}
\newcommand{\Catiocat}{\mathcal{C}_{IO}\xspace}
\newcommand{\Catckptcat}{\mathcal{C}_{Ckpt}\xspace}

\title{Optimal Cooperative Checkpointing for Shared High-Performance Computing Platforms
\thanks{NSF Award \#abcdef SMURFS.}
}

\author{
\IEEEauthorblockN{Dorian Arnold}
\IEEEauthorblockA{Emory University\\
Atlanta, GA, USA\\
\email{dorian.arnold@emory.edu}}
\and
\IEEEauthorblockN{George Bosilca}
\IEEEauthorblockA{Innovative Computing Laboratory\\
The University of Tennessee\\
Knoxville, TN, USA\\
\email{bosilca@icl.utk.edu}}
\and
\IEEEauthorblockN{Aurelien Bouteiller}
\IEEEauthorblockA{Innovative Computing Laboratory\\
The University of Tennessee\\
Knoxville, TN, USA\\
\email{bouteill@icl.utk.edu}}
\and
\IEEEauthorblockN{Kurt Feirrera}
\IEEEauthorblockA{Sandia National Laboratory, USA\\
\email{kbferre@sandia.gov}}
\and
\IEEEauthorblockN{Thomas Herault}
\IEEEauthorblockA{Innovative Computing Laboratory\\
The University of Tennessee\\
Knoxville, TN, USA\\
\email{herault@icl.utk.edu}}
\and
\IEEEauthorblockN{Yves Robert}
\IEEEauthorblockA{ENS Lyon, Lyon, France \&\\
The University of Tennessee Knoxville\\
Knoxville, TN, USA\\
\email{yves.robert@ens-lyon.fr}}
}


\begin{document}

\maketitle

\input{abstract.tex}
\input{intro.tex}
\input{model.tex}



\section{Algorithms}\label{sec:algorithms}
% Aurelien & George

\subsection{Scheduling applications}

\begin{itemize}
  \item We take a large number of applications. These applications belong to a few classes
  (say 4 classes). We enforce the percentage per class that is reported in the APEX paper.
  Each application comes with: class, number of processors, input I/O size, output I/O size,
  checkpoint size,  CPU time (equal to wall-time reported in APEX paper). To avoid side effects
  induced by having hundreds
  of completely identical applications,
  we use normal distributions for CPU time with mean equal to original APEX value and small standard deviation (say 10\%).
  \item We shuffle the applications and present them all in a big queue to the scheduler,
  which uses a simple first-fit algorithm. No user reservation, plain greedy. But we do assume the scheduler has access to the entire list of pending applications.
\item We also make the assumption that the applications know precisely
  the necessary wall-time. This diverges from a real scenario but
  allow us to only take in account one type of disturbance: an
  application is slower than expected due to C/R overheads (and will
  extend its original wall-time).
  \item When an application fails, we resubmit it with a new wall-time equal to the fraction that remains from the last checkpoint.  Input I/O becomes recovery. Output I/O is not modified.

We put back the restarted application at the head of the scheduling queue
  (so that it has a chance to finish it's previously allotted wall time
  immediately, given that the resources it previously occupied are
  presumably available), and force a reschedule with every application that
  has not started yet (already running applications are left to continue
  their work).
\end{itemize}

\subsection{Dealing with interference for non-cooperative application scheduling}
\begin{itemize}
  \item For coherence, we use the same policy for input I/O, output I/O and checkpoint operations \todo[inline]{we may want to separate Input+recovery from output+checkpoint (bidirectional channels)}
  \todo[inline]{answer: we could but not sure it is 100\% independent; and it would complicate things without changing the story.}
  \item Blocking: only one application can perform I/O operations at the given time-step.
  I/O requests are served on a FCFS basis. When an application has requested an I/O operation, it waits idle until being served.
  \item Sharing: the I/O resource is shared at every time-step to serve all concurrent requests. Sharing is fair, i.e., bandwidth is inversely proportional to number of requests.
  \item Here is an example:
  application $A_{1}$ wants to perform five gigabytes of I/O at time $t$,
  and available bandwidth is
  $1$ gigabyte per second. If there is no other request, the operation will take five seconds.
  Now application $A_{2}$ (with same number of processors as $A_{1}$, hence with same assigned bandwidth in the sharing policy) wants to perform two gigabytes of I/O at time $t+1$.
  \begin{itemize}
  \item Blocking: $A_{2}$'s operation is delayed until time $t+5$ and will complete at $t+7$.
  \item Sharing: Instead of taking $2$ seconds, $A_{2}$'s operation will take $4$ seconds, because the bandwidth is shared with $A_{1}$ during its whole transfer. As a results, $A_{2}$'s operation completes at $t+5$ while $A_{1}$'s  operation is slowed down during four units and now completes at $t+7$.
  \end{itemize}
\end{itemize}

\subsection{Orchestrating I/O for cooperative application scheduling}

\subsubsection{Strategy}

\begin{itemize}
  \item We always use the blocking strategy but not the FCFS policy.
  \item Instead, whenever an I/O operation completes at time $t$, we have a pool of application candidates:
  \begin{itemize}
   \item Category \IOcat $\Catiocat$: Applications $A_{i}$, $1\leq i \leq r$, which need to do input I/O, output I/O or recovery
  \item Category \Ckptcat $\Catckptcat$: Applications $A_{i}$, $r+1\leq i \leq r+s$,
  whose last checkpoint took place no later than time $t - \period{Daly}(A_{i})$, where $\period{Daly}(A_{i})$ is the Young/Daly period for $A_{i}$.
  \end{itemize}
  \item To decide which application is given priority among all $r+s$ candidates applications in $\Catiocat \cup \Catckptcat$, we select the one that minimizes the expected total waste induced by this choice, as explained below.
  \end{itemize}

 \subsubsection{Selection among candidate applications}

At the current time-step, there are $r+s$ candidates in $\Catiocat \cup \Catckptcat$:
\begin{itemize}
  \item Application $A_{i} \in \Catiocat$, $1\leq i \leq r$,
  has an I/O request of volume $v_{i}$ and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ initiated its I/O request $d_{i}$ seconds ago, and has been idle since $d_{i}$ seconds.
 \item Application $A_{i} \in  \Catckptcat$ has a checkpoint of duration $C_{i}$ seconds,
  and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ took its last checkpoint
  $d_{i}$ seconds ago, and keeps executing until it can checkpoint. For the record, we must have $d_{i} \geq \period{Daly}(A_{i})$
  since $A_{i}$ is a candidate.
  \end{itemize}

If we select application $A_{i}$ to perform I/O,  the expected waste $\wap{i}$ incurred
to the other $r+s-1$ candidate applications in  $\Catiocat \cup \Catckptcat$ is computed as follows.
Assume first that $A_{i} \in \Catiocat$. Then  $A_{i}$ will use the I/O resource for $v_{i}$ seconds.
\begin{itemize}
  \item Every other application $A_{j} \in \Catiocat$ will stay idle for $v_{i}$ additional seconds,
  hence its waste $\wapp{i}{j}$ is
  $$\wapp{i}{j} = q_{j} (d_{j} + v_{i})$$
  since there are $q_{j}$ processors enrolled in $A_{j}$ and idle for $d_{j} + v_{i}$ seconds. Note that for $A_{j} \in \Catiocat$, the waste $\wapp{i}{j}$ is deterministic.
  \item Every application $A_{j} \in \Catckptcat$ will continue executing for $v_{i}$ additional seconds, hence will be exposed to the risk of a failure that will strike within $v_{i}/2$ seconds on average. The probability of such a failure is $v_{i}/\mu_{j}$, where $\mu_{j}$ is the
  MTBF of application $A_{j}$. Since $A_{j}$ enrolls $q_{j}$ processors, we have $\mu_{j} = \muind/q_{j}$, where $\muind$ is the individual MTBF per processor. With this probability,
  the $q_{j}$ processors will have to recover and re-execute $d_{j} + v_{i}/2$ seconds of work,
  hence the waste $\wapp{i}{j}$ is
     $$\wapp{i}{j} = \frac{v_{i}}{\mu_{j} } q_{j} (R_{j} + d_{j} + \frac{v_{i}}{2}) =
     \frac{v_{i}}{\muind} q^{2}_{j} (R_{j} + d_{j} + \frac{v_{i}}{2})$$
     where $R_{j}$ is the recovery time for $A_{j}$.
Note that for $A_{j} \in \Catckptcat$, the waste $\wapp{i}{j}$ is probabilistic.
 \end{itemize}
 Altogether, the expected waste $\wap{i}$ incurred
to the other $r+s-1$ candidate applications is
$$\wap{i} = \sum_{A_{j} \in \Catiocat, j\neq i} \wapp{i}{j} + \sum_{A_{j} \in \Catckptcat} \wapp{i}{j}$$
We obtain
\begin{equation}
\label{eq.selection}
 \wap{i} = v_{i} \times \left( \sum_{1 \leq j \leq r, j\neq i} q_{j} (d_{j} + v_{i})
 + \sum_{r+1 \leq j \leq r+s}   \frac{q^{2}_{j}}{\muind} (R_{j} + d_{j} + \frac{v_{i}}{2}) \right)
\end{equation}

 Assume now that the selected application $A_{i} \in \Catckptcat$. Then  $A_{i}$ will use the I/O resource for $C_{i}$ seconds instead of $v_{i}$ seconds for $A_{i} \in \Catiocat$. We directly obtain the counterpart of Equation~\eqref{eq.selection} for its waste $\wap{i}$:
 \begin{equation}
\label{eq.selection2}
 \wap{i} = C_{i} \times \left( \sum_{1 \leq j \leq r} q_{j} (d_{j} + C_{i})
 + \sum_{r+1 \leq j \leq r+s, j\neq i}   \frac{q^{2}_{j}}{\muind} (R_{j} + d_{j} + \frac{C_{i}}{2}) \right)
\end{equation}

 Finally, we select the application $A_{i} \in \Catiocat \cup \Catckptcat$ whose waste
 $\wap{i}$ is minimal.


%\subsubsection{Selection in category \IOcat}
%\label{sec.iocat}
%
%  Let $(A_{i})_{1 \leq i \leq m}$ be the application candidates of category \IOcat.
%  Application $A_{i}$ has an I/O request of volume $v_{i}$ and enrolls $q_{i}$ processors.
%  Choosing $A_{i}$ makes every other candidate application $A_{j}$, $j \neq i$, keep $q_{j}$ processors idle during a time
%  proportional to $v_{i}$, so we choose $i$ that minimizes
%  $$(\sum_{j \neq i} q_{j}) \times v_{i}$$
%
%\subsubsection{Selection in category \Ckptcat}
%\label{sec.ckptcat}
%
%Let $(A_{i})_{1 \leq i \leq m}$ be the application candidates of category \Ckptcat.
%  Application $A_{i}$ has a checkpoint of duration $C_{i}$ seconds,
%  and enrolls $q_{i}$ processors. At the current time-step, $A_{i}$ took its last checkpoint
%  $d_{i}$ seconds ago (and for the record, we must have $d_{i} \geq \period{Daly}(A_{i})$
%  since $A_{i}$ is a candidate).
%   Choosing $A_{i}$ puts every other candidate application $A_{j}$, $j \neq i$,
%   at the risk of a failure that will strike within $C_{i}/2$ seconds on average.
%   Let $\bar{Q}_{i} = \sum_{j \neq i}q_{j}$ be the total number of
%   processors belonging to applications that want to checkpoint.
%   With probability $C_{i}/(\muind/\bar{Q}_{i})$ there will be a fault on one of these
%   processors. Here $\muind$ is the individual MTBF, hence we divide it by $\bar{Q}_{i}$
%   to get the MTBF over all processors at risk.
%
%   With probability $q_{j}/\bar{Q}_{i}$ the fault will strike application $A_{j}$ and incur a waste of duration $d_{j} + C_{i}/2$ for each of its $q_{j}$ processors. Altogether, the
%   expected amount of wasted time is
%   $$\frac{C_{i}}{(\muind/\bar{Q}_{i}) } \times \sum_{j \neq i}\frac{q_{j}}{\bar{Q}_{i}}(d_{j}+ \frac{C_{i}}{2})q_{j} = \frac{C_{i}}{\muind} \sum_{j \neq i} q_{j}^{2}(d_{j}+ \frac{C_{i}}{2})$$
%    and we choose $i$ that minimizes the above quantity.

\section{Lower Bound}\label{sec:lowerbound}
% Primary: Yves (does that go into a subsec of the algorithms or models?)


\section{Simulation}\label{sec:simulator}
% Primary: Thomas

In order to evaluate the performance of the proposed approach, we
ran a large set of discrete event simulations that we describe in this
section. Simulations are instantiated by a set of initial conditions
that define a set of application classes, and their distribution, and
on what platform these applications will execute.

\paragraph*{High level parameters}
Application classes are characterized by the following parameters:
size of initial input and output, size of checkpoints, quantity of
work to execute, number of nodes to use, quantity of diffuse I/O to
execute during the entire life of the application, and proportion of
the available CPU-hours to use overall.  Platforms are characterized
by the number of nodes, a system Mean Time Between Failures, and an
aggregated I/O subsystem bandwidth that is shared between the
different nodes.

A simulation first selects randomly an ordered set of applications
that are instances of the different application classes. The set of
applications is constrained by two parameters: the minimum duration to
execute this set of applications on that platform (not considering
interferences or faults), and the relative proportion of platform
resources allocated to each application class.

As an example, we consider the subset of application classes given by
the APEX workflows report for the subset of application classes of
LANL (EAP, LAP, Silverton and VPIC), simulated as running over the
Celio supercomputer, for a minimal execution time of 60h. A simulation
will randomly instantiate one of the four classes, assigning a work
duration uniformly distributed between $0.8w$ and $1.2w$, where $w$ is
the typical walltime specified for the chosen application class, and
count the resource allocated for this application class, until A) the
simulated execution would necessarily run for at least 2 months, and B) the
amount of resource used by the selected class is within 1\% of the
target goal of the representative workload percentage defined in the
APEX workflows report.

Once this set of applications is defined, a set of node failures dates
is computed. Dates of failures are placed following an exponential
distribution with the MTBF required for the simulation. The nodes that
are hit by failures are chosen uniformly between all nodes of the
simulated platform. The ordered set of applications and the date and
location of failures constitute the initial conditions of a simulation.

\paragraph*{Job Scheduling}
Then, an initial job schedule is computed: applications are set to
start and end at planned dates and on planned nodes, depending on
their characteristics, the job schedule and the order they appear in
the ordered set. The job schedule follows a simple first-fit
strategy. We simulate an online scheduling, and everytime an
application ends before its planned date (because of a failure, or
because it reached the end of its execution before the planned date),
the schedule is amended by de-scheduling all applications that were
not started yet and re-scheduling them following the current order of
applications.

\paragraph*{Execution Simulation}
Once an application is started, it first executes its initial
input. It then executes some work for a certain period and
checkpoints. These two steps are repeated until all planned work is
executed, after which the final output is executed by the application,
before it ends. At any time during the execution, a node hosting the
application may be subject to a failure (happening at pre-computed
dates and places). When that happens, this application is terminated,
and a new application is added to the list of applications to
schedule. That new application represents the restarting application:
it has similar characteristics as the failed application, but an
initial input corresponding to the checkpoint size, and a work time
corresponding to what was remaining to do at the last successful
checkpoint date. To reflect a frequent policy of platforms, this
restarting application is pushed in front of the ordered set of
applications to execute, so that it gets a higher priority to obtain
resources and complete its execution.

Checkpointing periods can either be dynamic or fixed. Many users
define a fixed period based on their own experience on the
platform. An ordinary heuristic is to take a checkpoint every hour,
with the reasoning that in the worst case, only 1h of work can be
lost. We also consider a dynamic period that is defined for each
application. We set for this the period given by the Young/Daly
approximations, that assumes that chekpoints last for a fixed
computable time. For this checkpoint time, we take the time it would
take to checkpoint the application if there was no interference.

\paragraph*{Interference Models}
Depending on the simulation, applications filesystem operations can
interfere with each other. The duration of an I/O, checkpoint storage
or checkpoint access is dynamically computed during the simulation. We
implemented 4 interference models: 1) a theoretically perfectly scalable
filesystem, where the duration is only function of the aggregated
bandwidth and the amount of data to transmit for a single application
(as if there was no inteference); 2) a simple interference model,
where all applications using the filesystem at a given time get a
portion of the available aggregated bandwidth proportional to the
number of nodes they use and inversely proportional to the number of
nodes involved for all applications doing I/O; 3) a first-come
first-served I/O scheduling, where applications are served in order,
the first receiving all the filesystem bandwidth until termination of
its execution, and the others waiting for their turn; and 4) an
ordering of the I/O scheduling similar to first-come first-served, but
implementing the prioriterizing described in
Section~\ref{sec:algorithms} (Cooperative I/O).

In the case of scheduled I/O (first-come first-served and Cooperative
I/O), initial inputs and final outputs are blocking (the application
cannot progress during the I/O until it is served), but checkpoints
are non blocking, and the application continues progressing its
computation until the filesystem is ready to serve the
checkpoint. Note that this means that if a failure hits the
application, it may have to re-execute from a checkpoint far in its
past, if it was not granted access to the filesystem for a long time.

\paragraph*{Method of statistics collection from simulations}
As we compare the four different filesystem sharing strategies, each
simulation is ran four times with the same initial conditions (ordered
set of applications and date and location of failures). Performance
statistics are collected over each simulation. In order to consolidate
these measures over many simulations, statistics are taken over a
segment of fixed length of each simulation (e.g. a segment of 48h of
simulation). This segment excludes both the first day the simulation
(in which applications might be synchronized artificially because a
subset starts at the same date), and the last day of the simulation
(in which a large amount of resource may not be used as new
applications are not added to the workload). We simulate a large
number of such runs (at least a thousand simulations per shown
measurement), and compute the first, second, third quartile of the
performance for each statistic, as well as the mean value.

\paragraph*{Workload} We consider the workload from LANL found
in the APEX Workflows report that consists of four simulation
applications: EAP, LAP, Silverton and VPIC. The main characteristics
of these classes are reported in Table~\ref{table:lanl}.

\begin{table}
\begin{tabular}{|l|c|c|c|c|}
\hline
 Workflow & EAP & LAP & Silverton & VPIC \\\hline
Workload percentage & 66 & 5.5 & 16.5 & 12 \\\hline
Work time (h) & 262.4 & 64 & 128 & 157.2 \\\hline
Number of cores & 16384 & 4096 & 32768 & 30000 \\\hline
Initial Input (\% of memory) &  3 & 5 & 70 & 10 \\\hline
Final Output (\% of memory) & 105 & 220 & 43 & 270 \\\hline
Checkpoint Size (\% of memory) & 160 & 185 & 350 & 85 \\\hline
\end{tabular}
\caption{LANL Workflow Workload from the APEX Workflows report\label{table:lanl}}
\end{table}

\section{Results}\label{sec:results}
% Primary: Thomas & all

\subsection{Existing Systems}

\subsection{Prospective Systems}


\section{Related Works}\label{sec:related}
% Primary: Kurt

We survey related work in this section. We first discuss papers related to I/O pressure
due to checkpointing. Then we deal with I/O interference.


\section{Concluding Remarks}\label{sec:conclusion}

In this paper we presented a comprehensive model to capture interference
between multiple applications performing fault-tolerance related I/O
on a shared HPC system. We proved that ... We designed multiple algorithms
to schedule and order the checkpointing I/O workload, with the intent of
diminishing the average slowdown sustained by applications on the
platform induced by sharing the I/O subsystem, \ie improve the throughput
of the platform. We designed a event-based simulator that permits
executing typical HPC workloads on current and prospective systems.
With this simulator we have been able to offer guidances as to the
prefered heuristics for scheduling checkpoint workloads, and the
general I/O requirements for future HPC systems to sustain checkpointing.


\bibliographystyle{IEEEtran}
\bibliography{biblio}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FOLLOWUP TEXT IS OLDER AND MAY NEED MERGING OR DISCARDING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section{Older text}



\subsection{Checkpointing and I/O}

For a single application, the optimal checkpointing period is given by the Young/Daly
formula~\cite{young74,daly04}. This period minimizes the platform waste, defined as
the fraction of the
execution time that does not contribute to the progress of the application (the
time \emph{wasted}).  There are two sources of waste, the time spent taking checkpoints
(which calls for longer checkpoint periods),
and the time needed to recover and re-execute after each failure
(which calls for shorter checkpoint periods),
The Young/Daly
period achieves the optimal trade-off between both sources to minimize the
total waste.
However, this optimal period may put too much pressure
on the I/O system. It is possible to use a longer, sub-optimal, period that would incur
less pressure and still lead to a reasonable waste. S. Arunagiri et al.~\cite{Arunagiri2009} have studied such trade-offs and they have shown, both analytically and instantiating the model with four real-life platforms,
that a great decrease in I/O requirement can be achieved  at the price of a small increase of the waste.

\subsection{I/O interference}


\section{Execution Model}
\label{sec.model}


\section{Optimal Checkpointing Period under I/O constraint}
\label{sec.optimal}

The waste of an application is the ratio of time that the application spends doing
resilience operations by the time that it does useful work. The time
spent doing resilience operations include the time spend during each period to checkpoint, and in case of failure, the time to rollback to the previous checkpoint, and the time to recompute lost work. We assume
that the rollback time is equivalent to the checkpoint time, and we
can define the waste $\wasteapp{i}$ of an application of class
$\app{i}$ as follows (\cite{springer-monograph}):

\begin{equation}
\wasteapp{i} = \wastefct{i}{\ckpt{i}} = \frac{\ckpt{i}}{\period{i}} +
\frac{\nbnodes{i}}{\mtbfplat}(\frac{\period{i}}{2} + \ckpt{i})
\label{eq.wasteAi}
\end{equation}

Let $\wasteplat$ be the waste of the platform. We define this as the
weighted arithmetic mean of the $\wasteapp{i}$ for all applications
(where each application is weighted by the number of computing nodes
it uses):

\begin{equation}
\wasteplat = \sum_i \frac{\nbapp{i} \nbnodes{i}}{\nbnodesplat} \wasteapp{i}
\label{eq.waste}
\end{equation}

In the absence of I/O constraints, the checkpointing period can be minimized
for each application independently. Indeed, the optimal period for an application
of class $\app{i}$ is obtained by minimizing $\wasteapp{i}$ in Equation~\eqref{eq.wasteAi}.
Differentiating and solving
$$\frac{\delta \wasteapp{i}}{\delta \period{i}} = - \frac{\ckpt{i}}{\period{i}^{2}} + \frac{\nbnodes{i}}{2 \mtbfplat} = 0$$
we readily derive that
\begin{equation}
\period{i} = \sqrt{2 \frac{\mtbfplat}{\nbnodes{i}} \ckpt{i}} = \sqrt{2 \mu_{i} \ckpt{i}}
\label{eq.daly}
\end{equation}
where $\mu_{i}$ is the MTBF of  class $\app{i}$ applications, which is the Young/Daly formula~\cite{young74,daly04}.
\todo[inline]{So they were right! We might want to define what $\period{i}$
  stands for before the reader reaches this point and can figure it by
  itself}

However, I/O constraints impose the use of sub-optimal periods. If each application
of  class $\app{i}$ checkpoints in time $\ckpt{i}$ during its period $\period{i}$ (hence without any contention), it uses the I/O device during a fraction $\frac{\ckpt{i}}{\period{i}}$ of the time.
The total usage fraction of the  I/O device is $\sum_{i} \frac{\nbapp{i} \ckpt{i}}{\period{i}}$
and cannot exceed $1$. Therefore, we have to solve the following optimization problem: find
the set of values $\period{i}$ that minimize $\wasteplat$ in Equation~\eqref{eq.waste} subject to the I/O constraint:

\begin{equation}
\sum_{i} \frac{\nbapp{i} \ckpt{i}}{\period{i}} \leq 1
\label{eq.IOconstraint}
\end{equation}

Note that Equation~\eqref{eq.IOconstraint} is a necessary condition, but is may not be sufficient:
even though the total I/O bandwidth is not exceeded, meaning there is enough capacity to take all the checkpoints at the given periods, we still need to orchestrate these checkpoints so that there is no contention. We come back to this point below.

The optimization problem writes: minimize $\wasteplat = \sum_i \frac{\nbapp{i} \nbnodes{i}}{\nbnodesplat}  \left( \frac{\ckpt{i}}{\period{i}} +
\frac{\nbnodes{i}}{\mtbfplat}(\frac{\period{i}}{2} + \ckpt{i}) \right)$
subject to $\ioconstraint = \sum_{i} \frac{\nbapp{i} \ckpt{i}}{\period{i}} \leq 1$.
Using the Karush-Kuhn-Tucker condition, we know that there exists a nonnegative constant
$\lambda$
such that
$$- \frac{\delta \wasteplat}{\delta \period{i}} = \lambda \frac{\delta\ioconstraint }{\delta \period{i}}$$
for all $i$. We derive that
$$\frac{\nbapp{i} \nbnodes{i} \ckpt{i}}{\nbnodesplat \period{i}^{2}} -    \frac{\nbapp{i} \nbnodes{i}^{2}}{2 \mtbfplat \nbnodesplat} = - \lambda \frac{\nbapp{i} \ckpt{i}}{\period{i}^{2}}
$$
for all $i$. This leads to:
 \begin{equation}
\period{i} = \sqrt{\frac{2 \mtbfplat  \nbnodesplat}{\nbnodes{i}^{2}} \left(\frac{\nbnodes{i}}{\nbnodesplat} +\lambda \right) \ckpt{i}}
  \label{eq.KKT}
\end{equation}
for all $i$. Note that when $\lambda=0$, Equation~\eqref{eq.KKT} reduces to Equation~\eqref{eq.daly}. Because of the I/O constraint in Equation~\eqref{eq.IOconstraint},
we will choose for $\lambda$ the minimum value such that Equation~\eqref{eq.IOconstraint}
  is satisfied. This will lead to periods $P_{i}$ larger than the optimal value of Equation~\eqref{eq.daly}.
  Note that there is no closed-form expression for the minimum value of $\lambda$,
  it has to be found numerically.
   Altogether, we state our main result:

   \begin{theorem}
  In the presence of I/O constraints, the optimal values of the checkpointing periods are given
  by Equation~\eqref{eq.KKT}, where $\lambda$ is the smallest nonnegative value such that
  Equation~\ref{eq.IOconstraint} holds.
\end{theorem}

\section{Optimal Cooperative Checkpointing Strategy}
\label{sec.strategy}

\subsection{With Burst Buffers}

We slightly change the machine model, and will consider that, in addition
to the global PFS, each node is provisioned with a local stage-in I/O
burst buffer. With burst buffers, $\ckpt{i}$ still represents the time it
takes to upload the checkpoint to the stable storage (the file system).
However, the availability of burst buffers permits a reduction in the
apparent time for the checkpoints as experienced by application idle
time. Note that under I/O constraints on the PFS, checkpoints may remain
in burst buffers (which is not a stable storage) until sufficient
PFS bandwidth is available.

Consider Algorithm~\ref{alg.withbb}: every $\period{i}$ time
units, each application of class $\app{i}$ takes a checkpoint and save
it onto the next free slot on the burst buffer; a global shared FIFO
queue transfers checkpoints from active slots in burst buffers onto
the parallel file system following the FIFO queue order.

\algblockdefx{Process}{EndProcess}[1]{\textbf{On Process} #1}{\textbf{End Process}}
\algnotext{EndProcess}
\algblockdefx{Every}{DoneEvery}[1]{\textbf{Every } #1}{\textbf{Done}}
\algnotext{DoneEvery}
\algblockdefx{When}{DoneWhen}[1]{\textbf{When } #1}{\textbf{Done}}
\algnotext{DoneWhen}
\algloopdefx{If}[1]{\textbf{If} #1 \textbf{then}}
\begin{algorithm}
\caption{Cooperative Checkpointing Algorithm with Burst Buffers}
\label{alg.withbb}
\begin{algorithmic}
\State \textbf{var} $transfer\_queue$, a FIFO initially empty
\Process{$p$, belonging to application $\application{i}{j}$ of class $\app{i}$}
   \State \textbf{var} $slots$ set of burst buffer files that can
   store a checkpoint
   \Every{$\period{i}$ time units}
           \State $slot  \gets $ oldest slot that is not being used for transfer
           \State Checkpoint application state into $slot$
           \If{$slot$ is marked done transferring}
                  \State Append $(p, slot)$ to $transfer\_queue$
    \DoneEvery
\EndProcess

\Process{$T$}
   \When{$transfer\_queue$ is not empty}
       \State Pop $slot$ from $transfer\_queue$
       \State Mark $slot$ as being transferred
       \State Transfer Checkpoint in $slot$ to File System
       \State Mark $slot$ as done transferring
   \DoneWhen
\EndProcess
\end{algorithmic}
\end{algorithm}

\begin{theorem}
  Algorithm~\ref{alg.withbb} ensures that all applications of class $\app{i}$
  checkpoint at most every $max(\sum_j\nbapp{j}*\ckpt{j}, \period{i})$
  time units, and requires only a burst buffer capable of storing 2
  checkpoints per process.
\end{theorem}

\begin{proof}
  \todo{This derives from $\sum_i \frac{\ckpt{i}}{\period{i}} \leq 1$,
    but should be done properly.}
\end{proof}

The theorem does provide a lower bound for the platform waste.\todo[inline]{the 2 storages holds only after the optimization, doesn't it? Or is it also a consequence of the above "proof" sketch?}
However, we have a FIFO system, and some applications may incur
a re-execution time larger than $P_{i}$. What is the worst case?
A first optimization is the following: when a checkpoint is taken
    by a given application, we check whether its previous checkpoint is
    still in the queue from the burst buffer to the file system. If yes, the new
checkpoint should \emph{replace} the old one, keeping the same position in the queue.
With this optimization, there is at most two checkpoints per application in the queue,
one being currently transferred and one waiting.
Then the worst case is to wait for the checkpoint of all the other applications.
Formally, the maximal re-execution time for application
$\application{i}{j}$ of class
$\app{i}$ is
$$\max(P_{i}, \sum_{k=1}^{\nbapps} n_{k}C_{k} - C_{i})$$

 \todo[inline]{Discussed at JLESC meeting: size of burst buffer can be
    bounded by 2 checkpoints easily, and it should be: if there are 3
    checkpoints in the burst buffer, then the first one might be being
    transferred, but this means that the 2nd is useless as we already
    reached the 3rd one. The 2nd should be discarded and its slot in
    the FIFO queue should be taken by the 3rd.}

  \todo[inline]{No clear what qualifies for burst buffers here, but
    currently the NVM bandwidth is (1) significantly lower than the
    network bandwidth, and (2) unidirectional. This might change in
    the future, but at least today it seems cheaper to use a
    buddy-checkpointing approach.}%


\subsection{Without Burst Buffers}

Without a local caching mechanism\footnote{Note that this case covers ``shared'' burst-buffers, where the
PFS contains an intermediate stage-in area (presumably with SSD drives, a much higher bandwidth, and possibly contention reduced to a subset of the nodes it serves, but provides for stable/persistent storage as soon as the data is committed to the shared burst-buffer.}, the times at which to take the checkpoint
must be scheduled to avoid any kind of checkpoint-checkpoint
interference (which can only waste resources, as all interfering
applications are slowed down while blocking on non-productive
operations). We consider here a centralized scheduler that decides at
any time what next application should checkpoint, and when.

The scheduler remembers when each application $\application{i}{j}$ of class
$\app{i}$ last initiated a checkpoint. We then define $\lastckpt{i}{j}$
as the time since the last checkpoint $\application{i}{j}$ started
(or since the start of $\application{i}{j}$ if none has been taken yet).
As soon as $\application{i}{j}$ has executed for $P_{i}$ time-units,
it is put in the checkpointing pool $\pool$ by the centralized scheduler. It
continues executing until it is selected from the pool to checkpoint.

Which application in the pool should be selected to checkpoint?
Assume that some previous checkpoint terminates at time $t$,
and let
$$\pool = \{ \application{i_{1}}{j_{1}}, \dots, \application{i_{k}}{j_{k}} \}$$
be the set of applications  in the pool at time $t$. All these applications are
candidate to checkpointing. At time $t$, each application $\application{i_{\ell}}{j_{\ell}} \in \pool$
has been executing for  $\lastckpt{i_{\ell}}{j_{\ell}}$ time-units.

If we select $\application{i_{\ell}}{j_{\ell}}$ to checkpoint (in time $C_{j_{\ell}}$),
and if there is a failure during that checkpoint, the time lost by every other application
$\application{i_{m}}{j_{m}} \in \pool$, $m \neq \ell$, is (in expectation) equal to
 $\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2}$.
We define the risk incurred by application
$\application{i_{m}}{j_{m}} \in \pool$, $m \neq \ell$
as its potential waste, which is the time lost divided by its MTBF $\frac{\mtbfplat}{\nbnodes{i}}$,
times the number $\nbnodes{i_m}$ processors enrolled by this application, i.e.
$$ \frac{\nbnodes{i_{m}}}{\mtbfplat}  (\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2})
\times \nbnodes{i_m} $$
Altogether, selecting $\application{i_{\ell}}{j_{\ell}}$ to checkpoint leads to a total risk
$$\risk(\application{i_{\ell}}{j_{\ell}}) = \sum_{1 \leq m \leq k, m \neq \ell} \frac{\nbnodes{i_{m}}^{2}}{\mtbfplat}  \times (\lastckpt{i_m}{j_m} + \frac{C_{j_{\ell}}}{2})$$
  for all applications that stayed in the pool.

We greedily choose the application in the pool that minimizes the risk:
$$\application{i_{\ell}}{j_{\ell}} = Argmin_{\application{i_m}{j_m} \in \pool} \risk(\application{i_m}{j_m})$$

%At the end of each checkpointing, the scheduler selects
%$\application{i}{j}$ such that
%$$
%\left\{
%\begin{array}{l}
%\nbnodes{i}(\frac{\wastefct{i}{\lastckpt{i}{j}}}{\wastefct{i}{\period{i}}}) \textrm{ if }\lastckpt{i}{j}\geq\period{i}\\
%0\textrm{ if }\lastckpt{i}{j}<\period{i}\\
%\end{array}\right.$$
%is maximal and strictly superior to 0.

Two remarks:
\begin{itemize}
\item Because we put applications in the pool only after they have run for $P_{i}$ times-steps,
this greedy algorithm guarantees Young/Daly periods to every application
whenever there is non conflict to access I/O resources.
\item The final schedule is not periodic. Instead, it is constructed dynamically
after each checkpoint completion. Computing the actual waste can be achieved
through simulation, and compared to the lower bound of Section~\ref{sec.optimal}.
\end{itemize}

\end{document}
