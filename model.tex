% !TEX root =  ipdps18.tex

\section{Model}
\label{sec:model}
% Primary: Yves

%\todo[inline]{Question: how can we consider applications with finite time, initial
%  input and final output? Current idea is to distribute complete
%  volume of I/O over wall time, then take a single fake schedule that
%  assign resources to the apps following a distribution, and say 'it
%  shouldn't be far from finite apps being scheduled eagerly over
%  finite resource for a long time, in average'.}



%TODO: moved out of "algorithms" as this is describing the model; My
% opinion is that the current text already captures these points, but
% please check
\begin{comment}
\begin{itemize}
\item We take a large number of applications. These applications belong to a few
  classes (say 4 classes). We enforce the percentage per class that is reported in
  the APEX paper.  Each application comes with: class, number of processors, input
  I/O size, output I/O size, checkpoint size, CPU time (equal to wall-time reported
  in APEX paper). To avoid side effects induced by having hundreds of completely
  identical applications, we use normal distributions for CPU time with mean equal to
  original APEX value and small standard deviation (say 10\%). \dca{Put in model
    section?}
  \item We shuffle the applications and present them all in a big queue to the scheduler,
  which uses a simple first-fit algorithm. No user reservation, plain greedy. But we do assume the scheduler has access to the entire list of pending applications.
\item We also make the assumption that the applications know precisely
  the necessary wall-time. This diverges from a real scenario but
  allow us to only take in account one type of disturbance: an
  application is slower than expected due to C/R overheads (and will
  extend its original wall-time). \dca{Put in model section?}
  \item When an application fails, we resubmit it with a new wall-time equal to the fraction that remains from the last checkpoint.  Input I/O becomes recovery. Output I/O is not modified.

We put back the restarted application at the head of the scheduling queue
  (so that it has a chance to finish it's previously allotted wall time
  immediately, given that the resources it previously occupied are
  presumably available), and force a reschedule with every application that
  has not started yet (already running applications are left to continue
  their work).
\end{itemize}
\end{comment}


In this work, we consider a shared platform that comprises a set of computational
nodes, storage resources in the form of a parallel file system, and a network that
interconnects the nodes as well as the storage resources. Applications are scheduled
on the platform by a job scheduler such that computational nodes are space-shared
(dedicated) amongst concurrent application instances. However, the I/O subsystem is
time-shared (contended) amongst the application instances.

Our \emph{application I/O} model entails loading an input file at startup, performing
regular I/O operations during their main execution phase, and finally dumping an
output file at completion. Because applications are long-running (typically, several
hours or days) and the platform is failure-prone, applications are protected using
coordinated CR that incur periodic \emph{CR I/O}.
Applications can vary in size (number of computational nodes), application I/O
pattern and requirements, and CR I/O requirements. To model these behavioral
variations with minimal parameters, we make the following simplifying assumptions
(that we validate in the experimental section):
\begin{compactitem}
  \item There is a large number of applications, but only a small number of
    application classes, i.e., sets of applications with similar duration, size,
    application I/O and CR I/O requirements;
  \item Other than initialization and finalization I/O, an application's (non-CR) I/O
    activity is evenly distributed over its entire makespan.
\end{compactitem}
We model the specific numbers and characteristics of the application classes based on
real benchmark data, such as the APEX benchmark on the Cielo platform~\cite{apex2016}.

To evaluate the scheduling policies, we consider a finite segment, typically lasting
a small number of days, of a representative schedule where the number of applications
in each class remains approximately constant at every instant. Of course, with
different application execution times, we cannot enforce a fixed proportion of each
application class at every instant. However, we ensure the proper proportion is
enforced in average throughout across schedule execution. Similarly, we enforce
that at every instant during the finite segment, at least 98\% of the nodes
are enrolled for the execution. This allows us to compare actual
(simulated) performance with the theoretical performance of a co-scheduling policy
that optimizes the steady-state I/O behavior of the application portfolio, assuming
that all processors are used.

Formally, we consider a set $\appset$ of $\nbapps$ applications classes
$\app{1}, \ldots \app{\nbapps}$ that execute concurrently on a platform with
$\nbnodesplat$ nodes. Application class $\app{i}$ specifies:
\begin{compactitem}
\item $\nbapp{i}$: the number of applications in $\app{i}$,
\item $\nbnodes{i}$: the number of nodes used by each application in $\app{i}$,
\item $\period{i}$: the checkpoint period of each application in $\app{i}$, and
\item $\ckpt{i}$ and $\reco{i}$: the checkpoint and recovery durations for each application in $\app{i}$ when there is no interference with other I/O operations.
\end{compactitem}
%
%$\nbapp{i}$ applications that each use
%$\nbnodes{i}$ nodes, and checkpoints
%periodically with period $\period{i}$, in a time $\ckpt{i}$ when there
%is no interference with other I/O operation.
%
At every instant, we schedule as many applications as possible.
In the theoretical analysis, we assume that
$\sum_{i}\nbapp{i} \nbnodes{i} = \nbnodesplat$, where $\nbnodesplat$ is the total
number of nodes in the platform.

%\todo[inline]{the portfolio of available and scheduled applications are not the same, no reason for available applications to match that constraints on node count, only on scheduled ones. }

%Consider a large-scale platform with several applications executing
%concurrently. All these applications routinely perform I/O operations
%throughout their execution. The average fraction of I/O bandwidth that
%remains available can be used for checkpointing. Ideally, each application $A_{i}$
%should checkpoint, during a time $C_{i}$ ,
%every $P_{i}$ units of time. Here $P_{i}$ is the length of the
%optimal checkpointing period given by the Young/Daly formula~\cite{young74,daly04}:
%$$P_{i} = \sqrt{2 \mu_{i} C_{i}}$$
%where $\mu_{i}$ is the application MTBF, which is inversely
%proportional to the number of processors enrolled in its execution.
%
%However, with each application having a different $P_{i}$, lasting and
%starting for and at arbitrary times (a behavior we call
%uncooperative), nothing prevents the checkpoint of an application to
%occur while another competitively does I/O (because of its normal
%application behavior, or because of a checkpoint). Because this
%introduces interferences between I/O (\cite{interference}), the time
%to complete both the checkpoint of the first application and the
%competing I/O of the second are adversely impacted. When many
%applications execute an I/O operation competitively, all of them can
%be impacted, reducing the efficiency of each.
%
%Moreover, checkpointing each application with optimal period $P_{i}$
%is possible only if enough I/O bandwidth is available. If this is the
%case, the impact of failures is kept to a minimum using Daly's period
%for each application.  However, if I/O bandwidth is limited, either in
%the absolute (imbalanced hardware design) or in the current
%co-execution (because a few applications need to consume a large
%fraction of bandwidth to progress), applications have to checkpoint
%less frequently.  All of them? if not, which ones? what are the
%optimal checkpointing periods in this context of co-scheduling with a
%given bound on available I/O bandwidth, and how to schedule the
%checkpoints in order to minimize interferences and optimize resource
%spent doing I/O? This paper answers these important questions.
